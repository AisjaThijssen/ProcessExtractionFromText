{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ChatGPT input for the improve references and improve flow prompts\n",
    "\n",
    "This code is used to create the ChatGPT input of the event list and flow from a single article (namely the article in the file `article_of_interest.txt`). If you want to change the article you create output for simply change the content of `article_of_interest.txt`. Copy the corrected event description from ChatGPT into the `chat_gpt_output_step_1` variable to create the flow inpt for ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Union, Optional, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.tokens.doc import Doc\n",
    "from itertools import combinations\n",
    "import graphviz\n",
    "\n",
    "# load spaCy Dutch model\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the data from a txt file splitting the text into Articles.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to where the data is located\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the name of the Article and its\n",
    "            content.\n",
    "    \"\"\"\n",
    "    with open(path, encoding=\"utf-8\") as file:\n",
    "        document_text = file.read()\n",
    "\n",
    "    # Split the document at newlines and remove empty newlines.\n",
    "    document_text = document_text.split(\"\\n\")\n",
    "    document_text = [sentence for sentence in document_text if sentence != \"\"]\n",
    "\n",
    "    documents = [[]]\n",
    "    i = 0\n",
    "\n",
    "    # Loop through the sentences and when you find an indicator of a new\n",
    "    # Article start a new entry in documents so we can store all sentences that\n",
    "    # belong to one Article together.\n",
    "    for sentence in document_text:\n",
    "        if bool(re.fullmatch(r\"(Artikel \\S+)( \\([^)]+\\))?\", sentence)):\n",
    "            documents.append(\n",
    "                [re.fullmatch(r\"(Artikel \\S+)( \\([^)]+\\))?\", sentence).group(1)]\n",
    "            )\n",
    "            i += 1\n",
    "        else:\n",
    "            documents[i].append(sentence)\n",
    "\n",
    "    # Remove the first document as it does not contain an Article but any\n",
    "    # information before the first article.\n",
    "    documents = documents[1:]\n",
    "\n",
    "    # Store the content of the Articles.\n",
    "    content = [\n",
    "        \" \".join(document[1:]).replace(\"\\t\", \" \")\n",
    "        for document in documents\n",
    "        if \"[Vervallen per\" not in \" \".join(document[1:]).replace(\"\\t\", \" \")\n",
    "    ]\n",
    "\n",
    "    # Store the names of the Articles\n",
    "    titles = [\n",
    "        document[0]\n",
    "        for document in documents\n",
    "        if \"[Vervallen per\" not in \" \".join(document[1:]).replace(\"\\t\", \" \")\n",
    "    ]\n",
    "\n",
    "    # Put everything into a DataFrame\n",
    "    data = pd.DataFrame({\"article\": titles, \"content\": content})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_sub_sentences(parse_tree: Doc) -> List[List[Token]]:\n",
    "    \"\"\"Extract all sub sentences from a main sentence using certain dependency\n",
    "    tags and the subtrees belonging to the words with those tags.\n",
    "\n",
    "    Args:\n",
    "        parse_tree (Doc): A SpaCy parsing tree of a full sentence including\n",
    "            dependencies that can be used to identify sub-sentences.\n",
    "\n",
    "    Returns:\n",
    "        List[List[Token]]: A list with all identified sub-sentences.\n",
    "    \"\"\"\n",
    "    sub_sentences = []\n",
    "\n",
    "    # Reconstruct the full sentence so we can check that the sub-sentence will\n",
    "    # not be the same a s the full sentence.\n",
    "    sentence = \"\".join(word.text_with_ws for word in parse_tree)\n",
    "\n",
    "    for word in parse_tree:\n",
    "        if word.dep_ in [\"acl\", \"acl:relcl\", \"advcl\", \"csubj\", \"ccomp\", \"xcomp\"]:\n",
    "            sub_sentence = \"\".join(w.text_with_ws for w in word.subtree)\n",
    "\n",
    "            # Identify if the sub-sentence is al least 3 words long and if the\n",
    "            # sub-sentence is really a sub-sentence (shorter than the full\n",
    "            # sentence).\n",
    "            if len(sub_sentence.split(\" \")) >= 3 and len(sub_sentence) < len(sentence):\n",
    "                sub_sentences.append([word for word in word.subtree])\n",
    "\n",
    "    return sub_sentences\n",
    "\n",
    "\n",
    "def identify_active_sentence(dependencies: List[str]) -> bool:\n",
    "    \"\"\"Identify if a sentence is written in active form or in passive form.\n",
    "\n",
    "    Args:\n",
    "        dependencies (List[str]): A list of all dependency tags of the words in\n",
    "            a sentence.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sentence is in active form and False if the sentence\n",
    "            is in passive form.\n",
    "    \"\"\"\n",
    "    for dependency in dependencies:\n",
    "        if \"pass\" in dependency:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_copula(word: Token) -> bool:\n",
    "    \"\"\"Identify if a word is part of a copula.\n",
    "\n",
    "    Args:\n",
    "        word (Token): The word we are looking at.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the words is part of a copula, False otherwise.\n",
    "    \"\"\"\n",
    "    # We say a word is part of a copula if the head is a form of to be and the\n",
    "    # word itself is its subject.\n",
    "    return word.head.lemma_ == \"zijn\" and word.dep_ == \"nsubj\"\n",
    "\n",
    "\n",
    "def create_actor(\n",
    "    actor: Tuple[Token, Optional[str]], tree: Doc, sentence_id: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"[Algorithm 7] This function creates an actor object from a word that is\n",
    "    indicated to be an actor.\n",
    "\n",
    "    Args:\n",
    "        actor (Tuple[Token, Optional[str]]): The main actor token found by the\n",
    "            algorithm.\n",
    "        tree (Doc): A parsing tree of the sentence.\n",
    "        sentence_id (int): The id of the sentence we are working on.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The original token, the modifiers relevant to that\n",
    "            token, the text representation of the actor, a boolean indicating\n",
    "            if the actor is the subject of the sentence (always set to True),\n",
    "            an actor that this actor is referencing to (always set to None),\n",
    "            and the id of the sentence this actor is in.\n",
    "    \"\"\"\n",
    "    relevant_words = {actor[0]}\n",
    "    new_words = True\n",
    "\n",
    "    while new_words:\n",
    "        # Find all noun specifiers based on the current relevant words and the\n",
    "        # dependency tags.\n",
    "        modifiers = {\n",
    "            word\n",
    "            for word in tree\n",
    "            if (\n",
    "                word.dep_\n",
    "                in [\n",
    "                    \"nmod:poss\",\n",
    "                    \"det\",\n",
    "                    \"amod\",\n",
    "                    \"iobj\",\n",
    "                    \"compound:prt\",\n",
    "                    \"xcomp\",\n",
    "                    \"nmod\",\n",
    "                    \"acl:relcl\",\n",
    "                    \"case\",\n",
    "                    \"fixed\",\n",
    "                    \"appos\",\n",
    "                ]\n",
    "            )\n",
    "            # Only include words that are related to a word we have so far but\n",
    "            # are not in our current list yet.\n",
    "            and (word.head in relevant_words) and (word not in relevant_words)\n",
    "        }\n",
    "\n",
    "        # If no new words are found we stop the loop.\n",
    "        if not modifiers:\n",
    "            new_words = False\n",
    "            continue\n",
    "\n",
    "        # Otherwise we add the words to the relevant words and repeat.\n",
    "        else:\n",
    "            relevant_words.update(modifiers)\n",
    "\n",
    "    actor_object = {\n",
    "        \"object\": actor,\n",
    "        \"relevant_words\": relevant_words,\n",
    "        \"text\": \" \".join(word.text.lower() for word in tree if word in relevant_words),\n",
    "        \"subject\": True,\n",
    "        \"reference\": None,\n",
    "        \"sentence_id\": sentence_id,\n",
    "    }\n",
    "    return actor_object\n",
    "\n",
    "\n",
    "def create_object(\n",
    "    object: Tuple[Token, Optional[str]], tree: Doc, sentence_id: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"[Algorithm 8] This function creates an object object from a word that is\n",
    "    indicated to be an object.\n",
    "\n",
    "    Args:\n",
    "        object (Tuple[Token, Optional[str]]): The main object token found by\n",
    "            the algorithm.\n",
    "        tree (Doc): A parsing tree of the sentence.\n",
    "        sentence_id (int): The id of the sentence we are working on.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The original token, the modifiers relevant to that\n",
    "            token, the text representation of the object, a boolean indicating\n",
    "            if the object is the subject of the sentence (always set to False),\n",
    "            an actor that this actor is referencing to (always set to None),\n",
    "            and the id of the sentence this object is in.\n",
    "    \"\"\"\n",
    "    # Create an actor out of the object.\n",
    "    object_object = create_actor(object, tree, sentence_id)\n",
    "    # Set subject to False to distinguish between actors and objects.\n",
    "    object_object[\"subject\"] = False\n",
    "    return object_object\n",
    "\n",
    "\n",
    "def create_action(\n",
    "    action: Tuple[Token, Optional[str]], tree: Doc, sentence_id: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"[Algorithm 10] This function creates an action object from a word that\n",
    "    is indicated to be an action.\n",
    "\n",
    "    Args:\n",
    "        action (Tuple[Token, Optional[str]]): The main action token found by\n",
    "            the algorithm.\n",
    "        tree (Doc): A parsing tree of the sentence.\n",
    "        sentence_id (int): The id of the sentence we are working on.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The original token, the modifiers relevant to that\n",
    "            token, the text representation of the action, the id of the\n",
    "            sentence this action is in, a list of markers found fot the action,\n",
    "            indicators for each marker if they are from a complementizer.\n",
    "    \"\"\"\n",
    "    relevant_words = {action[0]}\n",
    "    new_words = True\n",
    "\n",
    "    while new_words:\n",
    "        # Find all verb specifiers based on the current relevant words and the\n",
    "        # dependency tags.\n",
    "        modifiers = {\n",
    "            word\n",
    "            for word in tree\n",
    "            if (\n",
    "                word.dep_\n",
    "                in [\n",
    "                    \"aux\",\n",
    "                    \"aux:pass\",\n",
    "                    \"advcl\",\n",
    "                    \"advmod\",\n",
    "                    \"cop\",\n",
    "                    \"nmod\",\n",
    "                    \"acl:relcl\",\n",
    "                    \"case\",\n",
    "                    \"fixed\",\n",
    "                    \"xcomp\",\n",
    "                    \"mark\",\n",
    "                    \"obl\",\n",
    "                    \"det\",\n",
    "                    \"appos\",\n",
    "                    \"amod\",\n",
    "                ]\n",
    "            )\n",
    "            # Only include words that are related to a word we have so far but\n",
    "            # are not in our current list yet.\n",
    "            and (word.head in relevant_words) and (word not in relevant_words)\n",
    "        }\n",
    "\n",
    "        # If no new words are found we stop the loop.\n",
    "        if not modifiers:\n",
    "            new_words = False\n",
    "            continue\n",
    "\n",
    "        # Otherwise we add the words to the relevant words and repeat.\n",
    "        else:\n",
    "            relevant_words.update(modifiers)\n",
    "\n",
    "    action_object = {\n",
    "        \"object\": action,\n",
    "        \"relevant_words\": relevant_words,\n",
    "        \"text\": \" \".join(word.text.lower() for word in tree if word in relevant_words),\n",
    "        \"sentence_id\": sentence_id,\n",
    "        \"markers\": [],\n",
    "        \"marker_from_complementizer\": [],\n",
    "    }\n",
    "    return action_object\n",
    "\n",
    "\n",
    "def find_dependency(\n",
    "    dependency: str, tree: Doc, all: bool = False\n",
    ") -> Union[List[Token], Token, None]:\n",
    "    \"\"\"Find the word(s) with a certain dependency tag in a sentence if\n",
    "    available.\n",
    "\n",
    "    Args:\n",
    "        dependency (str): The dependency label we are looking for.\n",
    "        tree (Doc): A parsing tree of the sentence.\n",
    "        all (bool, optional): True if we want to find all occurrences of the\n",
    "            dependency label, False if we only want to find the first one.\n",
    "            Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Any[List[Token], Token, None]: A list of word tokens if we want to find\n",
    "            all occurrences of a dependency tag, one word token if we only want\n",
    "            to find the first occurrence of a dependency label and None if no\n",
    "            token with the given tag was found.\n",
    "    \"\"\"\n",
    "    if all:\n",
    "        dependencies = []\n",
    "        for word in tree:\n",
    "            if word.dep_ == dependency:\n",
    "                dependencies.append(word)\n",
    "        return dependencies\n",
    "    else:\n",
    "        # If all is False we only return the first occurrence of the relevant\n",
    "        # dependency.\n",
    "        for word in tree:\n",
    "            if word.dep_ == dependency:\n",
    "                return word\n",
    "    return None\n",
    "\n",
    "\n",
    "def check_conjunctions(\n",
    "    active: bool, tree: Doc, element: Token, element_type: str\n",
    ") -> List[Tuple[Token, str]]:\n",
    "    \"\"\"[Algorithm 5] Check if there are any conjunctions to the element given\n",
    "    as input.\n",
    "\n",
    "    Args:\n",
    "        active (bool): True if the sentence is in active form and False if the\n",
    "            sentence is in passive form.\n",
    "        tree (Doc): A parsing tree of the sentence.\n",
    "        element (Token): The token of the word of interest.\n",
    "        element_type (str): The label of the word of interest (action, actor or\n",
    "            object).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Token, str]]: A list of all conjunctions found with their\n",
    "            labels.\n",
    "    \"\"\"\n",
    "    conjunctions = []\n",
    "    # go through all words that are marked as conjunctions in the parsing tree\n",
    "    dependencies = find_dependency(\"conj\", tree, all=True)\n",
    "    for dependency in dependencies:\n",
    "        # Identify if the open clausal complement of the action is the governor\n",
    "        # of the conjunction.\n",
    "        xcomp_match = False\n",
    "        if element_type == \"action\":\n",
    "            xcomp = find_dependency(\"xcomp\", tree)\n",
    "            if xcomp:\n",
    "                if xcomp == dependency.head:\n",
    "                    xcomp_match = True\n",
    "\n",
    "        # If the element has a relation to the conjunction and the element is\n",
    "        # not part of a copula or when the above condition holds we need to\n",
    "        # create a new object.\n",
    "        if (\n",
    "            (element == dependency.head) and (not is_copula(dependency.head))\n",
    "        ) or xcomp_match:\n",
    "            conjunction_node = dependency\n",
    "            if xcomp_match:\n",
    "                new_element = element\n",
    "                new_element_type = \"action\"\n",
    "\n",
    "                # The line below is a part of the pseudocode but has no effect\n",
    "                # on the code here hence it is commented out.\n",
    "\n",
    "                # new_element_xcomp = conjunction_node\n",
    "            else:\n",
    "                if element_type == \"action\":\n",
    "                    new_element = conjunction_node\n",
    "                    new_element_type = \"action\"\n",
    "                else:\n",
    "                    new_element = conjunction_node\n",
    "                    new_element_type = \"object\"\n",
    "            conjunctions.append((new_element, new_element_type))\n",
    "\n",
    "            # To prevent infinite loops we make sure that the new element is\n",
    "            # not the same as the considered element.\n",
    "            if new_element != element:\n",
    "                conjunctions += check_conjunctions(\n",
    "                    active, tree, new_element, new_element_type\n",
    "                )\n",
    "\n",
    "    return conjunctions\n",
    "\n",
    "\n",
    "def determine_actors(active: bool, tree: Doc) -> List[Tuple[Token, str]]:\n",
    "    \"\"\"[Algorithm 3] Determine the actors present in a sentence.\n",
    "\n",
    "    Args:\n",
    "        active (bool): True if the sentence is in active form and False if the\n",
    "            sentence is in passive form.\n",
    "        tree (Doc): A parsing tree of the sentence.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Token, str]]: A list of all actors found with their labels.\n",
    "    \"\"\"\n",
    "    actors = []\n",
    "    # In an active sentence we look for the subject and in a passive sentence\n",
    "    # we look for the agent.\n",
    "    if active:\n",
    "        actor = find_dependency(\"nsubj\", tree)\n",
    "    else:\n",
    "        actor = find_dependency(\"obl:agent\", tree)\n",
    "\n",
    "    if actor:\n",
    "        actors.append((actor, \"actor\"))\n",
    "        actors += check_conjunctions(active, tree, actor, \"actor\")\n",
    "\n",
    "    return actors\n",
    "\n",
    "\n",
    "def determine_actions(active: bool, tree: Doc) -> List[Tuple[Token, str]]:\n",
    "    \"\"\"[Algorithm 4] Determine the actions present in a sentence.\n",
    "\n",
    "    Args:\n",
    "        active (bool): True if the sentence is in active form and False if the\n",
    "            sentence is in passive form.\n",
    "        tree (Doc): A parsing tree of the sentence.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Token, str]]: A list of all actions found with their labels.\n",
    "    \"\"\"\n",
    "    actions = []\n",
    "    main_predicate = None\n",
    "    if active:\n",
    "        subject = find_dependency(\"nsubj\", tree)\n",
    "        if subject:\n",
    "            main_predicate = subject.head\n",
    "\n",
    "            # In case the main verb is a copula verb we want to make sure that\n",
    "            # the main_predicate is set to the referent and not the form of to\n",
    "            # be as this gives more information.\n",
    "            cop = find_dependency(\"cop\", tree)\n",
    "            if cop:\n",
    "                if main_predicate == cop:\n",
    "                    main_predicate = cop.head\n",
    "\n",
    "        else:\n",
    "            object = find_dependency(\"obj\", tree)\n",
    "            if object:\n",
    "                main_predicate = object.head\n",
    "    else:\n",
    "        subject = find_dependency(\"nsubj:pass\", tree)\n",
    "\n",
    "        # We noticed in outputs where no nsubj:pass was found that looking for\n",
    "        # nsubj still have us the correct action.\n",
    "        if not subject:\n",
    "            subject = find_dependency(\"nsubj\", tree)\n",
    "\n",
    "        if subject:\n",
    "            main_predicate = subject.head\n",
    "\n",
    "    action = main_predicate\n",
    "    if action:\n",
    "        actions.append((action, \"action\"))\n",
    "        actions += check_conjunctions(active, tree, action, \"action\")\n",
    "\n",
    "    return actions\n",
    "\n",
    "\n",
    "def determine_objects(\n",
    "    active: bool, tree: Doc, action: Token\n",
    ") -> List[Tuple[Token, str]]:\n",
    "    \"\"\"[Algorithm 6] Determine the objects present in a sentence, belonging to\n",
    "    a certain action.\n",
    "\n",
    "    Args:\n",
    "        active (bool): True if the sentence is in active form and False if the\n",
    "            sentence is in passive form.\n",
    "        tree (Doc): A parsing tree of the sentence.\n",
    "        action (Token): The actions for which we want the object.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Token, str]]: A list of all objects found with their labels.\n",
    "    \"\"\"\n",
    "    objects = []\n",
    "    object_node = None\n",
    "\n",
    "    if active:\n",
    "        object = find_dependency(\"obj\", tree)\n",
    "        if object:\n",
    "            object_node = object\n",
    "        else:\n",
    "            cop = find_dependency(\"cop\", tree)\n",
    "            if cop:\n",
    "                if cop.head.dep_ == \"nsubj\":\n",
    "                    object_node = cop.head\n",
    "    else:\n",
    "        object_node = find_dependency(\"nsubj:pass\", tree)\n",
    "        # Similarly to when we were dealing with the action in the passive\n",
    "        # sentence we use nsubj if not nsubj:pass can be found.\n",
    "        if not object_node:\n",
    "            object_node = find_dependency(\"nsubj\", tree)\n",
    "\n",
    "    if object_node:\n",
    "        objects.append((object_node, \"object\"))\n",
    "        objects += check_conjunctions(active, tree, object_node, \"object\")\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "def extract_elements(tree: Doc, sentence_id: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"[Algorithm 2] Extract all actor, action and object combinations from a\n",
    "    sentence.\n",
    "\n",
    "    Args:\n",
    "        tree (Doc): A parsing tree of the input sentence.\n",
    "        sentence_id (int): The id of the sentence we are working on.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of all extracted combinations of an\n",
    "            action, actor and object.\n",
    "    \"\"\"\n",
    "    dependencies = [word.dep_ for word in tree if word.is_alpha]\n",
    "    # Identify if the sentence is active.\n",
    "    active = identify_active_sentence(dependencies)\n",
    "\n",
    "    # Find all actors and create actor object from them.\n",
    "    actors = determine_actors(active, tree)\n",
    "    actors = [create_actor(actor, tree, sentence_id) for actor in actors]\n",
    "\n",
    "    # Find all actions.\n",
    "    raw_actions = determine_actions(active, tree)\n",
    "\n",
    "    actions_with_object = []\n",
    "\n",
    "    # If no actions were found we append an empty tuple\n",
    "    if len(raw_actions) == 0:\n",
    "        actions_with_object.append((None, None))\n",
    "\n",
    "    # Loop over the actions and determine the objects for each of them.\n",
    "    for action in raw_actions:\n",
    "        objects = determine_objects(active, tree, action)\n",
    "        if len(objects) > 0:\n",
    "            # For each object action combination we create a new entry in the\n",
    "            # actions_with_objects list making the actions and objects into\n",
    "            # their respective objects.\n",
    "            for object in objects:\n",
    "                actions_with_object.append(\n",
    "                    (\n",
    "                        create_action(action, tree, sentence_id),\n",
    "                        create_object(object, tree, sentence_id),\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            # If no object was found save the action with a none object.\n",
    "            actions_with_object.append((create_action(action, tree, sentence_id), None))\n",
    "\n",
    "    final_actions = []\n",
    "    # Combine each action object pair with each actor found in the sentence.\n",
    "    for action in actions_with_object:\n",
    "        if action[0]:\n",
    "            if len(actors) > 0:\n",
    "                for actor in actors:\n",
    "                    final_actions.append(\n",
    "                        {\n",
    "                            \"action\": action[0].copy() if action[0] else None,\n",
    "                            \"actor\": actor.copy(),\n",
    "                            \"object\": action[1].copy() if action[1] else None,\n",
    "                            \"link\": None,\n",
    "                            \"link_type\": None,\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                final_actions.append(\n",
    "                    {\n",
    "                        \"action\": action[0].copy() if action[0] else None,\n",
    "                        \"actor\": None,\n",
    "                        \"object\": action[1].copy() if action[1] else None,\n",
    "                        \"link\": None,\n",
    "                        \"link_type\": None,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return final_actions\n",
    "\n",
    "\n",
    "def sentence_decomposition(\n",
    "    tree: Doc, sentence_id: int, completed_sentences: List[List[Token]] = []\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"[Algorithm 1] Split the sentence in sub-sentences and extract elements\n",
    "    for each sub-sentence.\n",
    "\n",
    "    Args:\n",
    "        tree (Doc): A parsing tree of the input sentence.\n",
    "        sentence_id (int): The id of the sentence we are working on.\n",
    "        completed_sentences (List[List[Token]]): A sentences already processed.\n",
    "            Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of all extracted combinations of an\n",
    "            action, actor and object for a sentence.\n",
    "    \"\"\"\n",
    "    extracted_elements = []\n",
    "\n",
    "    # Find the sub sentences\n",
    "    sub_sentences = extract_sub_sentences(tree)\n",
    "\n",
    "    # If there are not sub-sentences find the elements in the sentence.\n",
    "    if len(sub_sentences) == 0:\n",
    "        # Avoid infinite loops by only considering new sentences.\n",
    "        if tree not in completed_sentences:\n",
    "            completed_sentences.append(tree)\n",
    "            extracted_elements += extract_elements(tree, sentence_id)\n",
    "\n",
    "    # If there is a sub-sentence split the sub-sentence from the main sentence\n",
    "    # and extract elements from both.\n",
    "    elif len(sub_sentences) == 1:\n",
    "        extracted_elements += sentence_decomposition(sub_sentences[0], sentence_id)\n",
    "\n",
    "        # Identify the main sentence.\n",
    "        main_sentence = [word for word in tree if word not in sub_sentences[0]]\n",
    "        dependencies = [word.dep_ for word in main_sentence if word.is_alpha]\n",
    "        if any(\n",
    "            dep in dependencies for dep in [\"nsubj\", \"nsubj:pass\", \"obl:agent\", \"obj\"]\n",
    "        ):\n",
    "            extracted_elements += sentence_decomposition(main_sentence, sentence_id)\n",
    "\n",
    "    # If there are more than one sub-sentence extract elements from each\n",
    "    # sub-sentence and the remaining main sentence.\n",
    "    else:\n",
    "        remaining_sentence = tree\n",
    "\n",
    "        for sub_sentence in sub_sentences:\n",
    "            extracted_elements += sentence_decomposition(sub_sentence, sentence_id)\n",
    "\n",
    "            # Identify the words that remain.\n",
    "            remaining_sentence = [\n",
    "                word for word in remaining_sentence if word not in sub_sentence\n",
    "            ]\n",
    "\n",
    "        dependencies = [word.dep_ for word in remaining_sentence if word.is_alpha]\n",
    "\n",
    "        if any(\n",
    "            dep in dependencies for dep in [\"nsubj\", \"nsubj:pass\", \"obl:agent\", \"obj\"]\n",
    "        ):\n",
    "            extracted_elements += sentence_decomposition(\n",
    "                remaining_sentence, sentence_id, completed_sentences\n",
    "            )\n",
    "\n",
    "    return extracted_elements\n",
    "\n",
    "\n",
    "def choose_best_fit(\n",
    "    reference_results: List[Tuple[Dict[str, Any], float]]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Choose the reference that fits best according to the scores. If two\n",
    "    options are tied, choose the one the closest to the word that is bing\n",
    "    resolved.\n",
    "\n",
    "    Args:\n",
    "        reference_results (List[Tuple[Dict[str, Any], float]]): A list of all\n",
    "            referencing candidates and their score\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The best candidate\n",
    "    \"\"\"\n",
    "    # Extract all scores so we can find the best scoring.\n",
    "    scores = [result[1] for result in reference_results]\n",
    "\n",
    "    # If no candidates exist return None.\n",
    "    if len(scores) == 0:\n",
    "        return None\n",
    "\n",
    "    max_score = max(scores)\n",
    "    objects_with_highest_score = [\n",
    "        result[0] for result in reference_results if result[1] == max_score\n",
    "    ]\n",
    "\n",
    "    # If we have multiple results with the highest score choose the one with\n",
    "    # the highest sentence id, this will be the closest to the word we are\n",
    "    # resolving.\n",
    "    if len(objects_with_highest_score) > 1:\n",
    "        sentence_ids = [object[\"sentence_id\"] for object in objects_with_highest_score]\n",
    "        max_sentence_id = max(sentence_ids)\n",
    "        objects_with_highest_sentence_id = [\n",
    "            object\n",
    "            for object in objects_with_highest_score\n",
    "            if object[\"sentence_id\"] == max_sentence_id\n",
    "        ]\n",
    "        # In case there are multiple words with the same sentence id we return\n",
    "        # the last one as it will be closest to the word we are resolving.\n",
    "        return objects_with_highest_sentence_id[-1]\n",
    "    else:\n",
    "        return objects_with_highest_score[0]\n",
    "\n",
    "\n",
    "def find_reference(\n",
    "    sentence_id: int,\n",
    "    object_to_resolve: Dict[str, Any],\n",
    "    sentences: List[Dict[str, Any]],\n",
    "    distance_penalty: float = -15,\n",
    "    role_match_score: float = 20,\n",
    "    subject_role_score: float = 10,\n",
    "    object_role_score: float = 10,\n",
    "):\n",
    "    \"\"\"[Algorithm 12] Find the reference for the words that require one.\n",
    "\n",
    "    Args:\n",
    "        sentence_id (int): The id of the sentence that the word we are trying\n",
    "            to resolve is in.\n",
    "        object_to_resolve (Dict[str, Any]): the object we are trying to\n",
    "            resolve.\n",
    "        sentences (List[Dict[str, Any]]): All sentences and their extracted\n",
    "            events.\n",
    "        distance_penalty (float, optional): The penalty given to being far way\n",
    "            from the word we are resolving. Defaults to -15.\n",
    "        role_match_score (float, optional): The score we give to a word having\n",
    "            the right role (object or subject). Defaults to 20.\n",
    "        subject_role_score (float, optional): The score we give to a word if\n",
    "            the word to be resolved is not part of a copula and this word is\n",
    "            the actor in its event. Defaults to 10.\n",
    "        object_role_score (float, optional): The score we give to a word if the\n",
    "            word to be resolved is part of a copula and this word is the object\n",
    "            in its event. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: the chosen reference object.\n",
    "    \"\"\"\n",
    "    # we cannot find a reference for a word in the first sentence hence we\n",
    "    # return None.\n",
    "    if sentence_id == 0:\n",
    "        return None\n",
    "\n",
    "    # Create a list of all potential references out of all actors and objects\n",
    "    # that occur before the element to tbe resolved.\n",
    "    potential_references = []\n",
    "    for sentence in sentences[: sentence_id + 1]:\n",
    "        for event in sentence[\"events\"]:\n",
    "            for object in [\"actor\", \"object\"]:\n",
    "                if event[object]:\n",
    "                    # If it is in the same sentence it must have an index\n",
    "                    # before the element to be resolved.\n",
    "                    if event[object][\"sentence_id\"] == sentence_id:\n",
    "                        if (\n",
    "                            event[object][\"object\"][0].i\n",
    "                            < object_to_resolve[\"object\"][0].i\n",
    "                        ):\n",
    "                            potential_references.append(event[object])\n",
    "                    else:\n",
    "                        potential_references.append(event[object])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for object in potential_references:\n",
    "        # If an object already has a reference look to that reference instead.\n",
    "        while object.get(\"reference\"):\n",
    "            if object.get(\"reference\"):\n",
    "                object = object.get(\"reference\")\n",
    "\n",
    "        if object[\"object\"][1] == \"action\":\n",
    "            continue\n",
    "\n",
    "        # calculate the score based on the 4 aspects used.\n",
    "        score = 0\n",
    "        score += (sentence_id - object[\"sentence_id\"]) * distance_penalty\n",
    "        if is_copula(object_to_resolve[\"object\"][0]):\n",
    "            if object_to_resolve[\"subject\"] != object[\"subject\"]:\n",
    "                score += role_match_score\n",
    "            if not object[\"subject\"]:\n",
    "                score += object_role_score\n",
    "        else:\n",
    "            if object_to_resolve[\"subject\"] == object[\"subject\"]:\n",
    "                score += role_match_score\n",
    "            if object[\"subject\"]:\n",
    "                score += subject_role_score\n",
    "\n",
    "        results.append((object, score))\n",
    "\n",
    "    return choose_best_fit(results)\n",
    "\n",
    "\n",
    "def anaphora_resolution(\n",
    "    sentences: List[Dict[str, Any]], manual_resolution: Dict[str, str] = {}\n",
    "):\n",
    "    \"\"\"[Algorithm 11] Identify which objects need to be resolved and resolve\n",
    "    them.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str,Any]]): All sentences and their extracted\n",
    "            events.\n",
    "        manual_resolution (Dict[str, str], optional): A mapping of manual\n",
    "            resolution. Defaults to {}.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str,Any]]: The input sentences but with references added.\n",
    "    \"\"\"\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, event in enumerate(sentence[\"events\"]):\n",
    "            for object in [\"actor\", \"object\"]:\n",
    "                if not event[object]:\n",
    "                    continue\n",
    "                word = event[object][\"object\"][0]\n",
    "\n",
    "                # Evaluate if the word needs to be resolved. by identifying if\n",
    "                # it is a personal pronoun, a determiner or part of a\n",
    "                # pre-defined list.\n",
    "                if (\n",
    "                    (\"VNW\" in word.tag_)\n",
    "                    or (\"LID\" in word.tag_)\n",
    "                    or (word.text in [\"iemand\", \"iets\"])\n",
    "                ):\n",
    "                    # Apply the manual resolution if possible.\n",
    "                    if word.text in manual_resolution.keys():\n",
    "                        sentences[i][\"events\"][j][object][\"reference\"] = (\n",
    "                            manual_resolution[word.text]\n",
    "                        )\n",
    "                    else:\n",
    "                        # if the word is a determiner use the action before\n",
    "                        # this one as the reference.\n",
    "                        if \"LID\" in word.tag_:\n",
    "                            if j > 0:\n",
    "                                sentences[i][\"events\"][j][object][\"reference\"] = (\n",
    "                                    sentences[i][\"events\"][j - 1][\"action\"]\n",
    "                                )\n",
    "                            # If we have the first event of a sentence look for\n",
    "                            # the last event of the previous sentence.\n",
    "                            else:\n",
    "                                if i > 0:\n",
    "                                    index_last_event = (\n",
    "                                        len(sentences[i - 1][\"events\"]) - 1\n",
    "                                    )\n",
    "                                    if index_last_event >= 0:\n",
    "                                        sentences[i][\"events\"][j][object][\n",
    "                                            \"reference\"\n",
    "                                        ] = sentences[i - 1][\"events\"][\n",
    "                                            index_last_event\n",
    "                                        ][\n",
    "                                            \"action\"\n",
    "                                        ]\n",
    "                        # In any other case we use the resolution algorithm\n",
    "                        # above.\n",
    "                        else:\n",
    "                            sentences[i][\"events\"][j][object][\"reference\"] = (\n",
    "                                find_reference(i, event[object], sentences)\n",
    "                            )\n",
    "\n",
    "    # If the reference we found has a reference itself we assign the reference\n",
    "    # of the reference instead. This way a pronoun cannot refer to another\n",
    "    # pronoun for example.\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, event in enumerate(sentence[\"events\"]):\n",
    "            for object in [\"actor\", \"object\"]:\n",
    "                if not event[object]:\n",
    "                    continue\n",
    "                if sentences[i][\"events\"][j][object][\"reference\"]:\n",
    "                    if sentences[i][\"events\"][j][object][\"reference\"].get(\"reference\"):\n",
    "                        sentences[i][\"events\"][j][object][\"reference\"] = sentences[i][\n",
    "                            \"events\"\n",
    "                        ][j][object][\"reference\"][\"reference\"]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def extract_events_for_article(document: str) -> Dict[str, Any]:\n",
    "    \"\"\"Given an Article of the law extract all events in that Article.\n",
    "\n",
    "    Args:\n",
    "        document (str): The content of the Article of the law.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The extracted events form the Article.\n",
    "    \"\"\"\n",
    "    # Create a parsing tree.\n",
    "    parse_tree = nlp(document)\n",
    "\n",
    "    # Split the document into sentences.\n",
    "    sentences = [\n",
    "        \"\".join(w.text_with_ws for w in sentence) for sentence in parse_tree.sents\n",
    "    ]\n",
    "    information_for_each_sentence = []\n",
    "\n",
    "    # Extract element for each sentence and save all information\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tree = nlp(sentence)\n",
    "        extracted_elements = sentence_decomposition(sentence_tree, i)\n",
    "        sentence_information = {\n",
    "            \"sentence_id\": i,\n",
    "            \"sentence\": sentence,\n",
    "            \"sentence_tree\": sentence_tree,\n",
    "            \"events\": extracted_elements,\n",
    "        }\n",
    "        information_for_each_sentence.append(sentence_information)\n",
    "\n",
    "    # Resolve references.\n",
    "    final_sentences = anaphora_resolution(information_for_each_sentence)\n",
    "\n",
    "    return final_sentences\n",
    "\n",
    "\n",
    "def generate_chat_gpt_input(\n",
    "    sentence_information: List[Dict[str, Any]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Print the event descriptions with the actor, action, and object text and\n",
    "    references. This information is used as input to the improvement step done\n",
    "    by ChatGPT.\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): The processed sentences\n",
    "        with all events.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str,Any]]: The ChatGPT input.\n",
    "    \"\"\"\n",
    "    final_output = []\n",
    "    for sentence in sentence_information:\n",
    "        # for each event extract the event description, the actor, action and\n",
    "        # object text and the references\n",
    "        for event in sentence[\"events\"]:\n",
    "            # get action text\n",
    "            action = {\"text\": event[\"action\"][\"text\"]}\n",
    "            if event[\"actor\"]:\n",
    "                # get actor text and reference\n",
    "                if event[\"actor\"][\"reference\"]:\n",
    "                    actor = {\n",
    "                        \"text\": event[\"actor\"][\"text\"],\n",
    "                        \"reference\": event[\"actor\"][\"reference\"][\"text\"],\n",
    "                    }\n",
    "                else:\n",
    "                    actor = {\"text\": event[\"actor\"][\"text\"], \"reference\": None}\n",
    "            else:\n",
    "                actor = None\n",
    "            if event[\"object\"]:\n",
    "                # get object text and reference\n",
    "                if event[\"object\"][\"reference\"]:\n",
    "                    object = {\n",
    "                        \"text\": event[\"object\"][\"text\"],\n",
    "                        \"reference\": event[\"object\"][\"reference\"][\"text\"],\n",
    "                    }\n",
    "                else:\n",
    "                    object = {\"text\": event[\"object\"][\"text\"], \"reference\": None}\n",
    "            else:\n",
    "                object = None\n",
    "\n",
    "            object_reference = None\n",
    "            actor_reference = None\n",
    "\n",
    "            # Get the event description in the same we we do when we create the\n",
    "            # flowchart, so excluding marks and replacing objects and actors\n",
    "            # with their reference if there is one.\n",
    "\n",
    "            if event[\"object\"]:\n",
    "                if event[\"object\"][\"reference\"]:\n",
    "                    object_reference = event[\"object\"][\"reference\"][\"text\"]\n",
    "                object_text = [\n",
    "                    word\n",
    "                    for word in event[\"object\"][\"relevant_words\"]\n",
    "                    if word.dep_ != \"mark\"\n",
    "                ]\n",
    "            else:\n",
    "                object_text = []\n",
    "\n",
    "            if event[\"actor\"]:\n",
    "                if event[\"actor\"][\"reference\"]:\n",
    "                    actor_reference = event[\"actor\"][\"reference\"][\"text\"]\n",
    "                actor_text = [\n",
    "                    word\n",
    "                    for word in event[\"actor\"][\"relevant_words\"]\n",
    "                    if word.dep_ != \"mark\"\n",
    "                ]\n",
    "            else:\n",
    "                actor_text = []\n",
    "\n",
    "            action_text = [\n",
    "                word\n",
    "                for word in event[\"action\"][\"relevant_words\"]\n",
    "                if word.dep_ != \"mark\"\n",
    "            ]\n",
    "\n",
    "            all_relevant_words = list(\n",
    "                set(list(object_text) + list(actor_text) + list(action_text))\n",
    "            )\n",
    "            # make sure the words are all in the same order as they are in the original text\n",
    "            all_relevant_words.sort(key=lambda word: word.i)\n",
    "\n",
    "            # replace the object and actor with the word they refer to in the text.\n",
    "            if object_reference:\n",
    "                all_relevant_words[all_relevant_words.index(list(object_text)[0])] = (\n",
    "                    object_reference\n",
    "                )\n",
    "            if actor_reference:\n",
    "                all_relevant_words[all_relevant_words.index(list(actor_text)[0])] = (\n",
    "                    actor_reference\n",
    "                )\n",
    "\n",
    "            # put everything in lowercase\n",
    "            all_relevant_words = [\n",
    "                word.text.lower() if not isinstance(word, str) else word\n",
    "                for word in all_relevant_words\n",
    "            ]\n",
    "            event_text = \" \".join(all_relevant_words)\n",
    "\n",
    "            new_event = {\n",
    "                \"event_description\": event_text,\n",
    "                \"action\": action,\n",
    "                \"actor\": actor,\n",
    "                \"object\": object,\n",
    "            }\n",
    "            final_output.append(new_event)\n",
    "    # print the output so it can be copied and fed to ChatGPT\n",
    "    print(final_output)\n",
    "    return final_output\n",
    "\n",
    "\n",
    "def store_results_per_article(sentences: Dict, output_path: str) -> None:\n",
    "    \"\"\"Store intermediate output into a Word file for feedback.\n",
    "\n",
    "    Args:\n",
    "        sentences (Dict): The processed sentences with all relevant information\n",
    "        output_path (str): the path where to save the output.\n",
    "    \"\"\"\n",
    "    article_document = Document()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        article_document.add_heading(f\"Sentence {sentence['sentence_id']}\", level=3)\n",
    "\n",
    "        sentence_paragraph = article_document.add_paragraph()\n",
    "        actor_words = []\n",
    "        action_words = []\n",
    "        object_words = []\n",
    "        for event in sentence[\"events\"]:\n",
    "            if event[\"action\"]:\n",
    "                action_words += event[\"action\"][\"relevant_words\"]\n",
    "            if event[\"actor\"]:\n",
    "                actor_words += event[\"actor\"][\"relevant_words\"]\n",
    "            if event[\"object\"]:\n",
    "                object_words += event[\"object\"][\"relevant_words\"]\n",
    "\n",
    "        # Put action words in red, actor words in green and object words in\n",
    "        # blue. All other words are simply printed in black.\n",
    "        for word in sentence[\"sentence_tree\"]:\n",
    "            if word in action_words:\n",
    "                sentence_paragraph.add_run(word.text_with_ws).font.color.rgb = RGBColor(\n",
    "                    255, 0, 0\n",
    "                )\n",
    "            elif word in actor_words:\n",
    "                sentence_paragraph.add_run(word.text_with_ws).font.color.rgb = RGBColor(\n",
    "                    0, 255, 0\n",
    "                )\n",
    "            elif word in object_words:\n",
    "                sentence_paragraph.add_run(word.text_with_ws).font.color.rgb = RGBColor(\n",
    "                    0, 0, 255\n",
    "                )\n",
    "            else:\n",
    "                sentence_paragraph.add_run(word.text_with_ws)\n",
    "\n",
    "        # Below each sentence print an overview of all extracted events\n",
    "        if not (\n",
    "            (len(sentence[\"events\"]) == 1) and (not sentence[\"events\"][0][\"action\"])\n",
    "        ):\n",
    "            event_paragraph = article_document.add_paragraph()\n",
    "            event_paragraph.add_run(\"Extracted events:\")\n",
    "        event_list = None\n",
    "        for event in sentence[\"events\"]:\n",
    "            if event[\"action\"]:\n",
    "                event_list = article_document.add_paragraph(style=\"List Number\")\n",
    "                event_list.add_run(\"Action: \" + event[\"action\"][\"text\"])\n",
    "            if event[\"actor\"]:\n",
    "                if not event_list:\n",
    "                    event_list = article_document.add_paragraph(style=\"List Number\")\n",
    "                event_list.add_run(\", actor: \" + event[\"actor\"][\"text\"])\n",
    "                if event[\"actor\"][\"reference\"]:\n",
    "                    event_list.add_run(f\" ({event['actor']['reference']['text']})\")\n",
    "            if event[\"object\"]:\n",
    "                if not event_list:\n",
    "                    event_list = article_document.add_paragraph(style=\"List Number\")\n",
    "                event_list.add_run(\", object: \" + event[\"object\"][\"text\"])\n",
    "                if event[\"object\"][\"reference\"]:\n",
    "                    event_list.add_run(f\" ({event['object']['reference']['text']})\")\n",
    "\n",
    "    article_document.save(output_path)\n",
    "\n",
    "\n",
    "def find_action(relation: Token, event_list: List[Dict[str, any]]) -> Optional[int]:\n",
    "    \"\"\"Find the action belonging to the extracted relation.\n",
    "\n",
    "    Args:\n",
    "        relation (Token): the relation that should be related to an action\n",
    "        event_list (List[Dict[str, any]]): a list of all events in the sentence\n",
    "            containing the relation.\n",
    "\n",
    "    Returns:\n",
    "        Optional[int]: the index of the action belonging to the relation.\n",
    "    \"\"\"\n",
    "    # Get a list of all actions.\n",
    "    actions = [event[\"action\"] for event in event_list]\n",
    "    for k, action in enumerate(actions):\n",
    "        if action:\n",
    "            # We are looking for the actions that are the parent of the\n",
    "            # relation token.\n",
    "            if relation.head == action[\"object\"][0]:\n",
    "                return k\n",
    "\n",
    "\n",
    "def marker_detection(\n",
    "    sentence_information: List[Dict[str, Any]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"[Algorithm 13] Detect one word markers in the text and attach them to\n",
    "    the associated actions.\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): The sentences to analyse\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The input sentences with markers added to the\n",
    "            actions.\n",
    "    \"\"\"\n",
    "    for i, sentence in enumerate(sentence_information):\n",
    "        # find all markers with the mark dependency\n",
    "        marks = find_dependency(\"mark\", sentence[\"sentence_tree\"], all=True)\n",
    "        for mark in marks:\n",
    "            relevant_action = find_action(mark, sentence[\"events\"])\n",
    "            if relevant_action is not None:\n",
    "                sentence_information[i][\"events\"][relevant_action][\"action\"][\n",
    "                    \"markers\"\n",
    "                ].append(mark.text.lower())\n",
    "                # If the action is apart of a clausal complement we want to\n",
    "                # indicate that the marker comes from a complementizer.\n",
    "                if mark.head.dep_ == \"ccomp\":\n",
    "                    sentence_information[i][\"events\"][relevant_action][\"action\"][\n",
    "                        \"marker_from_complementizer\"\n",
    "                    ].append(True)\n",
    "                else:\n",
    "                    sentence_information[i][\"events\"][relevant_action][\"action\"][\n",
    "                        \"marker_from_complementizer\"\n",
    "                    ].append(False)\n",
    "\n",
    "        # Find all markers with the advmod dependency\n",
    "        mods = find_dependency(\"advmod\", sentence[\"sentence_tree\"], all=True)\n",
    "        for mod in mods:\n",
    "            relevant_action = find_action(mod, sentence[\"events\"])\n",
    "            if relevant_action is not None:\n",
    "                sentence_information[i][\"events\"][relevant_action][\"action\"][\n",
    "                    \"markers\"\n",
    "                ].append(mod.text.lower())\n",
    "                sentence_information[i][\"events\"][relevant_action][\"action\"][\n",
    "                    \"marker_from_complementizer\"\n",
    "                ].append(False)\n",
    "\n",
    "    return sentence_information\n",
    "\n",
    "\n",
    "def detect_compound_indicators(\n",
    "    sentence_information: List[Dict[str, Any]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"[Algorithm 14] detect compound indicators of connections between actions\n",
    "    using keyword lists.\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): The sentences to analyse.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: the input sentences markers added for the\n",
    "            compound indicators.\n",
    "    \"\"\"\n",
    "    # Define the stop word lists.\n",
    "    conditional_indicators = [\n",
    "        \"als\",\n",
    "        \"hetzij\",\n",
    "        \"of\",\n",
    "        \"indien\",\n",
    "        \"geval\",\n",
    "        \"anders\",\n",
    "        \"optioneel\",\n",
    "        \"wanneer\",\n",
    "    ]\n",
    "\n",
    "    parallel_indicators = [\n",
    "        \"terwijl\",\n",
    "        \"tussentijd\",\n",
    "        \"parallel\",\n",
    "        \"gelijktijdig\",\n",
    "        \"ondertussen\",\n",
    "        \"tegelijkertijd\",\n",
    "    ]\n",
    "\n",
    "    sequence_indicators = [\n",
    "        \"dan\",\n",
    "        \"na\",\n",
    "        \"nadien\",\n",
    "        \"daarna\",\n",
    "        \"vervolgens\",\n",
    "        \"basis\",\n",
    "        \"gebaseerd\",\n",
    "        \"dus\",\n",
    "    ]\n",
    "\n",
    "    for i, sentence in enumerate(sentence_information):\n",
    "        actions = [event[\"action\"] for event in sentence[\"events\"]]\n",
    "        for j, action in enumerate(actions):\n",
    "            if action:\n",
    "                # Go through all words relevant to the action and check if\n",
    "                # there are any matches with the words in the stop word lists.\n",
    "                relevant_words = [\n",
    "                    word.text.lower() for word in action[\"relevant_words\"]\n",
    "                ]\n",
    "\n",
    "                # add c to the marker saved so we can distinguish what marker\n",
    "                # came from where.\n",
    "                if set(relevant_words).intersection(conditional_indicators):\n",
    "                    sentence_information[i][\"events\"][j][\"action\"][\"markers\"].append(\n",
    "                        \"als c\"\n",
    "                    )\n",
    "                    sentence_information[i][\"events\"][j][\"action\"][\n",
    "                        \"marker_from_complementizer\"\n",
    "                    ].append(True)\n",
    "\n",
    "                elif set(relevant_words).intersection(sequence_indicators):\n",
    "                    sentence_information[i][\"events\"][j][\"action\"][\"markers\"].append(\n",
    "                        \"dan c\"\n",
    "                    )\n",
    "                    sentence_information[i][\"events\"][j][\"action\"][\n",
    "                        \"marker_from_complementizer\"\n",
    "                    ].append(False)\n",
    "\n",
    "                elif set(relevant_words).intersection(parallel_indicators):\n",
    "                    sentence_information[i][\"events\"][j][\"action\"][\"markers\"].append(\n",
    "                        \"terwijl c\"\n",
    "                    )\n",
    "                    sentence_information[i][\"events\"][j][\"action\"][\n",
    "                        \"marker_from_complementizer\"\n",
    "                    ].append(False)\n",
    "\n",
    "    return sentence_information\n",
    "\n",
    "\n",
    "def determine_conjunct_events(sentence: Dict[str, Any], event_id: int) -> List[int]:\n",
    "    \"\"\"This function finds all conjunct events that are connected to the input\n",
    "    event within the input sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (Dict[str, Any]): The input sentence where conjunctions can\n",
    "            be found\n",
    "        event_id (int): The id within the sentence of the event for which we\n",
    "            want to find conjunct events\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list with the id's of the conjunct events.\n",
    "    \"\"\"\n",
    "    conjunct_indeces = []\n",
    "    sentence_tree = sentence[\"sentence_tree\"]\n",
    "    dependencies = find_dependency(\"conj\", sentence_tree, all=True)\n",
    "    for dependency in dependencies:\n",
    "        for element in [\"action\", \"actor\", \"object\"]:\n",
    "            if sentence[\"events\"][event_id][element]:\n",
    "                # if an element in the event of interest is connected to the\n",
    "                # conjunction dependency we try to find the corresponding event.\n",
    "                if (\n",
    "                    sentence[\"events\"][event_id][element][\"object\"][0]\n",
    "                    == dependency.head\n",
    "                ):\n",
    "                    for k, event in enumerate(sentence[\"events\"]):\n",
    "                        if event[element]:\n",
    "                            # In the corresponding event the same element should be\n",
    "                            # the word associated with the conjunction dependency.\n",
    "                            if event[element][\"object\"][0] == dependency:\n",
    "                                conjunct_indeces.append(k)\n",
    "\n",
    "    return conjunct_indeces\n",
    "\n",
    "\n",
    "def add_implicit_markers(\n",
    "    sentence_information: List[Dict[str, Any]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"[Algorithm 15] Add implicit markers to the list of markers for each\n",
    "    activity.\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): All information relevant\n",
    "            to each sentence in a document.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The input data with implicit markers added.\n",
    "    \"\"\"\n",
    "    for i, sentence in enumerate(sentence_information):\n",
    "        next_marker = None\n",
    "        actions = [event[\"action\"] for event in sentence[\"events\"]]\n",
    "        for j, action in enumerate(actions):\n",
    "            # if we found an if-type statement before we add then then (dan) to\n",
    "            # the next event as an if statement is logically followed by a then.\n",
    "            if next_marker:\n",
    "                sentence_information[i][\"events\"][j][\"action\"][\"markers\"].append(\n",
    "                    next_marker\n",
    "                )\n",
    "                sentence_information[i][\"events\"][j][\"action\"][\n",
    "                    \"marker_from_complementizer\"\n",
    "                ].append(False)\n",
    "                next_marker = None\n",
    "            if sentence_information[i][\"events\"][j][\"action\"]:\n",
    "                if sentence_information[i][\"events\"][j][\"action\"][\"markers\"] != []:\n",
    "                    # see if there is an if-type statement using a stop word\n",
    "                    # list of conditional indicators.\n",
    "                    if set(\n",
    "                        sentence_information[i][\"events\"][j][\"action\"][\"markers\"]\n",
    "                    ).intersection(\n",
    "                        {\n",
    "                            \"als\",\n",
    "                            \"hetzij\",\n",
    "                            \"of\",\n",
    "                            \"indien\",\n",
    "                            \"geval\",\n",
    "                            \"terwijl\",\n",
    "                            \"anders\",\n",
    "                            \"optioneel\",\n",
    "                            \"wanneer\",\n",
    "                            \"als c\",\n",
    "                        }\n",
    "                    ):\n",
    "                        next_marker = \"dan\"\n",
    "                    # If we find a sequential indicator (using a stop word list)\n",
    "                    # then we make sure all actions of conjunct events get the\n",
    "                    # same markers as this one.\n",
    "                    if set(\n",
    "                        sentence_information[i][\"events\"][j][\"action\"][\"markers\"]\n",
    "                    ).intersection(\n",
    "                        {\n",
    "                            \"dan\",\n",
    "                            \"na\",\n",
    "                            \"nadien\",\n",
    "                            \"daarna\",\n",
    "                            \"vervolgens\",\n",
    "                            \"basis\",\n",
    "                            \"gebaseerd\",\n",
    "                            \"dus\",\n",
    "                            \"dan c\",\n",
    "                        }\n",
    "                    ):\n",
    "                        conjunct_actions = determine_conjunct_events(sentence, j)\n",
    "                        for conjunct_action in conjunct_actions:\n",
    "                            if conjunct_action != j:\n",
    "                                # find what markers this action has that are\n",
    "                                # not already part of action of the conjunct\n",
    "                                # event.\n",
    "                                markers_to_add = [\n",
    "                                    marker\n",
    "                                    for marker in action[\"markers\"]\n",
    "                                    if marker\n",
    "                                    not in sentence_information[i][\"events\"][\n",
    "                                        conjunct_action\n",
    "                                    ][\"action\"][\"markers\"]\n",
    "                                ]\n",
    "                                from_complementizer_to_add = [\n",
    "                                    marker_from_complementizer\n",
    "                                    for k, marker_from_complementizer in enumerate(\n",
    "                                        action[\"marker_from_complementizer\"]\n",
    "                                    )\n",
    "                                    if action[\"markers\"][k]\n",
    "                                    not in sentence_information[i][\"events\"][\n",
    "                                        conjunct_action\n",
    "                                    ][\"action\"][\"markers\"]\n",
    "                                ]\n",
    "\n",
    "                                sentence_information[i][\"events\"][conjunct_action][\n",
    "                                    \"action\"\n",
    "                                ][\"markers\"] += markers_to_add\n",
    "\n",
    "                                sentence_information[i][\"events\"][conjunct_action][\n",
    "                                    \"action\"\n",
    "                                ][\n",
    "                                    \"marker_from_complementizer\"\n",
    "                                ] += from_complementizer_to_add\n",
    "\n",
    "    return sentence_information\n",
    "\n",
    "\n",
    "def correct_order(sentence_information: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"[Algorithm 16] Fix the ordering of the events in case there is an if\n",
    "    clause following a main clause.\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): All information relevant\n",
    "            to each sentence in a document.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: All information relevant to each sentence in a\n",
    "            document with corrected event ordering.\n",
    "    \"\"\"\n",
    "    conditional_indicators = [\n",
    "        \"als\",\n",
    "        \"hetzij\",\n",
    "        \"of\",\n",
    "        \"indien\",\n",
    "        \"geval\",\n",
    "        \"anders\",\n",
    "        \"optioneel\",\n",
    "        \"wanneer\",\n",
    "        \"als c\",\n",
    "    ]\n",
    "\n",
    "    for i, sentence in enumerate(sentence_information):\n",
    "        for j, event in enumerate(sentence[\"events\"]):\n",
    "            if event[\"action\"]:\n",
    "                if event[\"action\"][\"markers\"] != []:\n",
    "                    swap_events = False\n",
    "                    # if we find a conditional indicator among the markers and\n",
    "                    # this marker does not come from a complementizer we swap\n",
    "                    # events\n",
    "                    if set(event[\"action\"][\"markers\"]).intersection(\n",
    "                        conditional_indicators\n",
    "                    ):\n",
    "                        for indicator in conditional_indicators:\n",
    "                            if indicator in event[\"action\"][\"markers\"]:\n",
    "                                if not event[\"action\"][\"marker_from_complementizer\"][\n",
    "                                    event[\"action\"][\"markers\"].index(indicator)\n",
    "                                ]:\n",
    "                                    swap_events = True\n",
    "                    if swap_events:\n",
    "                        to_swap = sentence[\"events\"][j - 1], sentence[\"events\"][j]\n",
    "                        (\n",
    "                            sentence_information[i][\"events\"][j],\n",
    "                            sentence_information[i][\"events\"][j - 1],\n",
    "                        ) = to_swap\n",
    "\n",
    "    return sentence_information\n",
    "\n",
    "\n",
    "def find_action_with_object(\n",
    "    sentence_information: List[Dict[str, Any]], object: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Given an object  or actor of an action find the corresponding event.\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): A list with information on\n",
    "            all sentences in the document.\n",
    "        object (Dict[str, Any]): The object or actor for which we have to find\n",
    "            the event\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The event containing the object or actor.\n",
    "    \"\"\"\n",
    "    for sentence in sentence_information:\n",
    "        for event in sentence[\"events\"]:\n",
    "            if (event[\"object\"] == object) or (event[\"actor\"] == object):\n",
    "                return event\n",
    "\n",
    "    # If no event was found return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def can_be_merged(\n",
    "    event_1: Dict[str, Any], event_2: Dict[str, Any], ignore_negation_modifier: bool\n",
    ") -> bool:\n",
    "    \"\"\"This function checks if two events can be merged based on 4 conditions\n",
    "    (5 if ignore_negation_modifier is False): both have no or the same marker,\n",
    "    one of the actions is weak, one of the actors is missing or requires a\n",
    "    reference, one of the objects is missing or needs a reference(, and if\n",
    "    ignore_negation_modifier is False we also check if both do or both do not\n",
    "    have a negation modifier).\n",
    "\n",
    "    Args:\n",
    "        event_1 (Dict[str, Any]): One of the events that is checked for merging.\n",
    "        event_2 (Dict[str, Any]): One of the events that is checked for merging.\n",
    "        ignore_negation_modifier (bool): If True we do not look at the negation\n",
    "            modifier.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the two actions can be merged according to the applied\n",
    "            conditions and False otherwise.\n",
    "    \"\"\"\n",
    "    relevant_words_event_1 = [\n",
    "        word.text.lower() for word in event_1[\"action\"][\"relevant_words\"]\n",
    "    ]\n",
    "    relevant_words_event_2 = [\n",
    "        word.text.lower() for word in event_2[\"action\"][\"relevant_words\"]\n",
    "    ]\n",
    "\n",
    "    # Check for negation modifiers if ignore_negation_modifier is False\n",
    "    if not ignore_negation_modifier:\n",
    "        event_1_negation = [\n",
    "            word\n",
    "            for word in relevant_words_event_1\n",
    "            if word in [\"niet\", \"geen\", \"nooit\", \"niemand\", \"nergens\", \"niets\"]\n",
    "        ]\n",
    "        event_2_negation = [\n",
    "            word\n",
    "            for word in relevant_words_event_2\n",
    "            if word in [\"niet\", \"geen\", \"nooit\", \"niemand\", \"nergens\", \"niets\"]\n",
    "        ]\n",
    "        if ((len(event_1_negation) < 0) and (len(event_2_negation) > 0)) or (\n",
    "            (len(event_1_negation) > 0) and (len(event_2_negation) < 0)\n",
    "        ):\n",
    "            return False\n",
    "\n",
    "    action_1 = event_1[\"action\"]\n",
    "    action_2 = event_2[\"action\"]\n",
    "\n",
    "    # Check if both have the same marker or no markers\n",
    "\n",
    "    no_common_markers = (\n",
    "        len([marker for marker in action_1[\"markers\"] if marker in action_2[\"markers\"]])\n",
    "        == 0\n",
    "    )\n",
    "    at_least_one_marker = (len(action_1[\"markers\"]) > 0) or (\n",
    "        len(action_2[\"markers\"]) > 0\n",
    "    )\n",
    "\n",
    "    if no_common_markers and at_least_one_marker:\n",
    "        return False\n",
    "\n",
    "    # check for weak verbs using a stop word list getting the lemma of the\n",
    "    # verbs in the action.\n",
    "\n",
    "    lemmas_verbs_event_1 = [\n",
    "        word.lemma for word in event_1[\"action\"][\"relevant_words\"] if \"WW\" in word.tag_\n",
    "    ]\n",
    "    lemmas_verbs_event_2 = [\n",
    "        word.lemma for word in event_2[\"action\"][\"relevant_words\"] if \"WW\" in word.tag_\n",
    "    ]\n",
    "\n",
    "    weak_verbs = [\n",
    "        \"zijn\",\n",
    "        \"hebben\",\n",
    "        \"doen\",\n",
    "        \"berijken\",\n",
    "        \"starten\",\n",
    "        \"beginnen\",\n",
    "        \"bestaan\",\n",
    "        \"baseren\",\n",
    "    ]\n",
    "\n",
    "    event_1_is_weak = set(lemmas_verbs_event_1).issubset(weak_verbs)\n",
    "    event_2_is_weak = set(lemmas_verbs_event_2).issubset(weak_verbs)\n",
    "\n",
    "    if (event_1_is_weak and event_2_is_weak) or (\n",
    "        not event_1_is_weak and not event_2_is_weak\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    # Check if actors are missing or need to be resolved\n",
    "\n",
    "    event_1_no_actor = event_1[\"actor\"] is None\n",
    "    event_2_no_actor = event_2[\"actor\"] is None\n",
    "\n",
    "    if not event_1_no_actor:\n",
    "        event_1_actor_needs_reference = (\n",
    "            True if event_1.get(\"actor\", {}).get(\"reference\") else False\n",
    "        )\n",
    "    else:\n",
    "        event_1_actor_needs_reference = False\n",
    "\n",
    "    if not event_2_no_actor:\n",
    "        event_2_actor_needs_reference = (\n",
    "            True if event_2.get(\"actor\", {}).get(\"reference\") else False\n",
    "        )\n",
    "    else:\n",
    "        event_2_actor_needs_reference = False\n",
    "\n",
    "    # The line below  makes use of the fact that if event_1_no_actor is True\n",
    "    # event_1_needs_reference is always False and the same for event 2.\n",
    "    if (event_1_no_actor == event_1_actor_needs_reference) == (\n",
    "        event_2_no_actor == event_2_actor_needs_reference\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    # Check if objects are missing or need to be resolved\n",
    "\n",
    "    event_1_no_object = event_1.get(\"object\") is None\n",
    "    event_2_no_object = event_2.get(\"object\") is None\n",
    "\n",
    "    if not event_1_no_object:\n",
    "        event_1_object_needs_reference = (\n",
    "            True if event_1.get(\"object\", {}).get(\"reference\") else False\n",
    "        )\n",
    "    else:\n",
    "        event_1_object_needs_reference = False\n",
    "\n",
    "    if not event_2_no_object:\n",
    "        event_2_object_needs_reference = (\n",
    "            True if event_2.get(\"object\", {}).get(\"reference\") else False\n",
    "        )\n",
    "    else:\n",
    "        event_2_object_needs_reference = False\n",
    "\n",
    "    if (event_1_no_object == event_1_object_needs_reference) == (\n",
    "        event_2_no_object == event_2_object_needs_reference\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    # If all above conditions hold and we did not find a rule that does not\n",
    "    # apply we can return True\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def merge_actions(\n",
    "    event_1: Dict[str, Any], event_2: Dict[str, Any]\n",
    ") -> Tuple[Dict[str, Any], bool]:\n",
    "    \"\"\"[Algorithm 18] Merge two events into one.\n",
    "\n",
    "    Args:\n",
    "        event_1 (Dict[str, Any]): The first event to merge.\n",
    "        event_2 (Dict[str, Any]): The second event to merge.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Any], bool]: The new event and an indicator if the\n",
    "            second event should be removed (True) or if the first event should\n",
    "            be removed (False).\n",
    "    \"\"\"\n",
    "    # Identify which event has the weak action assuming that one of the two\n",
    "    # must have it based on the conditions stated in the can be merged function.\n",
    "    lemmas_verbs_event_1 = [\n",
    "        word.lemma for word in event_1[\"action\"][\"relevant_words\"] if \"WW\" in word.tag_\n",
    "    ]\n",
    "\n",
    "    weak_verbs = [\n",
    "        \"zijn\",\n",
    "        \"hebben\",\n",
    "        \"doen\",\n",
    "        \"berijken\",\n",
    "        \"starten\",\n",
    "        \"beginnen\",\n",
    "        \"bestaan\",\n",
    "        \"baseren\",\n",
    "    ]\n",
    "\n",
    "    event_1_is_weak = set(lemmas_verbs_event_1).issubset(weak_verbs)\n",
    "\n",
    "    if event_1_is_weak:\n",
    "        weak_action = event_1\n",
    "        strong_action = event_2\n",
    "        remove_event_2 = False\n",
    "    else:\n",
    "        weak_action = event_2\n",
    "        strong_action = event_1\n",
    "        remove_event_2 = True\n",
    "\n",
    "    # If the strong event has no actor or an actor that needs to be resolved\n",
    "    # copy the one from the weak event if it exists.\n",
    "\n",
    "    weak_no_actor = weak_action[\"actor\"] is None\n",
    "    strong_no_actor = strong_action[\"actor\"] is None\n",
    "\n",
    "    if not weak_no_actor:\n",
    "        weak_actor_needs_reference = (\n",
    "            True if weak_action.get(\"actor\", {}).get(\"reference\") else False\n",
    "        )\n",
    "    else:\n",
    "        weak_actor_needs_reference = False\n",
    "\n",
    "    if not strong_no_actor:\n",
    "        strong_actor_needs_reference = (\n",
    "            True if strong_action.get(\"actor\", {}).get(\"reference\") else False\n",
    "        )\n",
    "    else:\n",
    "        strong_actor_needs_reference = False\n",
    "\n",
    "    if (strong_no_actor and not weak_no_actor) or (\n",
    "        strong_actor_needs_reference and not weak_actor_needs_reference\n",
    "    ):\n",
    "        strong_action[\"actor\"] = weak_action[\"actor\"]\n",
    "\n",
    "    # If the strong event has no object or an object that needs to be resolved\n",
    "    # copy the one from the weak event if it exists.\n",
    "\n",
    "    weak_no_object = weak_action[\"object\"] is None\n",
    "    strong_no_object = strong_action[\"object\"] is None\n",
    "\n",
    "    if not weak_no_object:\n",
    "        weak_object_needs_reference = (\n",
    "            True if weak_action.get(\"object\", {}).get(\"reference\") else False\n",
    "        )\n",
    "    else:\n",
    "        weak_object_needs_reference = False\n",
    "\n",
    "    if not strong_no_object:\n",
    "        strong_object_needs_reference = (\n",
    "            True if strong_action.get(\"object\", {}).get(\"reference\") else False\n",
    "        )\n",
    "    else:\n",
    "        strong_object_needs_reference = False\n",
    "\n",
    "    if (strong_no_object and not weak_no_object) or (\n",
    "        strong_object_needs_reference and not weak_object_needs_reference\n",
    "    ):\n",
    "        strong_action[\"object\"] = weak_action[\"object\"]\n",
    "\n",
    "    return strong_action, remove_event_2\n",
    "\n",
    "\n",
    "def get_event_from_action(\n",
    "    sentence_information: List[Dict[str, Any]], action: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Find an event given its action\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): A list with information on\n",
    "            all sentences in a document.\n",
    "        action (Dict[str, Any]): The action for which we are trying to find the\n",
    "            event.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The event containing the input action.\n",
    "    \"\"\"\n",
    "    for sentence in sentence_information:\n",
    "        for event in sentence[\"events\"]:\n",
    "            if event[\"action\"] == action:\n",
    "                return event\n",
    "\n",
    "\n",
    "def combine_actions(sentence_information: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"[Algorithm 17] Check if two actions can be combined when there is a\n",
    "    reference relationship between them, and if so merge them or copy\n",
    "    information from one to the other.\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): A list with information on\n",
    "            all sentences in the document.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The updated sentence information.\n",
    "    \"\"\"\n",
    "    # Loop over list copies so when elements are removed the loop stays intact.\n",
    "    for i, sentence in enumerate(sentence_information[:]):\n",
    "        for j, event in enumerate(sentence[\"events\"][:]):\n",
    "            # Events can only be combined if one event is has a reference to an\n",
    "            # element of another event. Hence we identify if there is another\n",
    "            # event that we are referring to.\n",
    "            referenced_action = None\n",
    "            if event[\"actor\"]:\n",
    "                # check if the actor refers to an action\n",
    "                if event[\"actor\"][\"reference\"]:\n",
    "                    if event[\"actor\"][\"reference\"][\"object\"][1] == \"action\":\n",
    "                        referenced_action = get_event_from_action(\n",
    "                            sentence_information, event[\"actor\"][\"reference\"]\n",
    "                        )\n",
    "            if not referenced_action:\n",
    "                if event[\"object\"]:\n",
    "                    # Check if the object refers to an action, actor or object.\n",
    "                    if event[\"object\"][\"reference\"]:\n",
    "                        if event[\"object\"][\"reference\"][\"object\"][1] == \"action\":\n",
    "                            referenced_action = get_event_from_action(\n",
    "                                sentence_information, event[\"object\"][\"reference\"]\n",
    "                            )\n",
    "                        else:\n",
    "                            referenced_action = find_action_with_object(\n",
    "                                sentence_information, event[\"object\"][\"reference\"]\n",
    "                            )\n",
    "\n",
    "            if referenced_action:\n",
    "                # store indexes to properly adapt the information if needed\n",
    "                referenced_sentence_index = referenced_action[\"action\"][\"sentence_id\"]\n",
    "                referenced_action_index = sentence_information[\n",
    "                    referenced_sentence_index\n",
    "                ][\"events\"].index(referenced_action)\n",
    "                if can_be_merged(event, referenced_action, True):\n",
    "                    # merge the two events into one and remove the \"weak\" one.\n",
    "                    merged_action, remove_reference = merge_actions(\n",
    "                        event, referenced_action\n",
    "                    )\n",
    "                    if remove_reference:\n",
    "                        sentence_information[i][\"events\"][j] = merged_action\n",
    "                        sentence_information[referenced_sentence_index][\n",
    "                            \"events\"\n",
    "                        ].remove(referenced_action)\n",
    "                    else:\n",
    "                        sentence_information[referenced_sentence_index][\"events\"][\n",
    "                            referenced_action_index\n",
    "                        ] = merged_action\n",
    "                        sentence_information[i][\"events\"].remove(event)\n",
    "                elif can_be_merged(event, referenced_action, False):\n",
    "                    # Copy actor and object information between events.\n",
    "                    if event[\"actor\"] and not referenced_action[\"actor\"]:\n",
    "                        referenced_action[\"actor\"] = event[\"actor\"]\n",
    "                    elif not event[\"actor\"] and referenced_action[\"actor\"]:\n",
    "                        event[\"actor\"] = referenced_action[\"actor\"]\n",
    "                    if event[\"object\"] and not referenced_action[\"object\"]:\n",
    "                        referenced_action[\"object\"] = event[\"object\"]\n",
    "                    elif not event[\"object\"] and referenced_action[\"object\"]:\n",
    "                        event[\"object\"] = referenced_action[\"object\"]\n",
    "\n",
    "                    sentence_information[i][\"events\"][j] = event\n",
    "                    sentence_information[referenced_sentence_index][\"events\"][\n",
    "                        referenced_action_index\n",
    "                    ] = referenced_action\n",
    "\n",
    "    return sentence_information\n",
    "\n",
    "\n",
    "def is_linkable(event_1: Dict[str, Any], event_2: Dict[str, Any]) -> bool:\n",
    "    \"\"\"This function determines if two actions are linkable based on 6 elements\n",
    "    that need to match between the two: the copula specifier, the negation\n",
    "    status, the actor, the object, the open clausal complement, and\n",
    "    prepositional specifiers whose head word is \"naar\" or \"over\".\n",
    "\n",
    "    Args:\n",
    "        event_1 (Dict[str, Any]): The first event\n",
    "        event_2 (Dict[str, Any]): The second event\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the two events can be linked, False otherwise.\n",
    "    \"\"\"\n",
    "    # check the copula specifier\n",
    "    important_action_words_1 = event_1[\"action\"][\"relevant_words\"]\n",
    "    important_action_words_2 = event_2[\"action\"][\"relevant_words\"]\n",
    "\n",
    "    copula_specifier_1 = [\n",
    "        word.head.text.lower()\n",
    "        for word in important_action_words_1\n",
    "        if word.dep_ == \"cop\"\n",
    "    ]\n",
    "    copula_specifier_2 = [\n",
    "        word.head.text.lower()\n",
    "        for word in important_action_words_2\n",
    "        if word.dep_ == \"cop\"\n",
    "    ]\n",
    "\n",
    "    if copula_specifier_1 != copula_specifier_2:\n",
    "        return False\n",
    "\n",
    "    # Check the negation status (taken from the can_be_merged_function)\n",
    "    event_1_negation = [\n",
    "        word.text.lower()\n",
    "        for word in important_action_words_1\n",
    "        if word.text.lower() in [\"niet\", \"geen\", \"nooit\", \"niemand\", \"nergens\", \"niets\"]\n",
    "    ]\n",
    "    event_2_negation = [\n",
    "        word.text.lower()\n",
    "        for word in important_action_words_2\n",
    "        if word.text.lower() in [\"niet\", \"geen\", \"nooit\", \"niemand\", \"nergens\", \"niets\"]\n",
    "    ]\n",
    "    if ((len(event_1_negation) < 0) and (len(event_2_negation) > 0)) or (\n",
    "        (len(event_1_negation) > 0) and (len(event_2_negation) < 0)\n",
    "    ):\n",
    "        return False\n",
    "\n",
    "    # Check if they have the same actor\n",
    "    if event_1[\"actor\"]:\n",
    "        actor_1 = event_1[\"actor\"][\"text\"]\n",
    "    else:\n",
    "        actor_1 = None\n",
    "    if event_2[\"actor\"]:\n",
    "        actor_2 = event_2[\"actor\"][\"text\"]\n",
    "    else:\n",
    "        actor_2 = None\n",
    "\n",
    "    if actor_1 != actor_2:\n",
    "        return False\n",
    "\n",
    "    # Check if they have the same object\n",
    "    if event_1[\"object\"]:\n",
    "        object_1 = event_1[\"object\"][\"text\"]\n",
    "    else:\n",
    "        object_1 = None\n",
    "    if event_2[\"object\"]:\n",
    "        object_2 = event_2[\"object\"][\"text\"]\n",
    "    else:\n",
    "        object_2 = None\n",
    "\n",
    "    if object_1 != object_2:\n",
    "        return False\n",
    "\n",
    "    # Find out if the xcomp elements match\n",
    "    xcomp_event_1 = [word for word in important_action_words_1 if word.dep_ == \"xcomp\"]\n",
    "    xcomp_event_2 = [word for word in important_action_words_2 if word.dep_ == \"xcomp\"]\n",
    "\n",
    "    if xcomp_event_1 != xcomp_event_2:\n",
    "        return False\n",
    "\n",
    "    # Find prepositional specifiers whose head word is to or about using subtrees\n",
    "    prep_specifier_subtree_1 = [\n",
    "        word.head.subtree\n",
    "        for word in important_action_words_1\n",
    "        if word.text.lower() in [\"naar\", \"over\"]\n",
    "    ]\n",
    "    prep_specifier_subtree_2 = [\n",
    "        word.head.subtree\n",
    "        for word in important_action_words_2\n",
    "        if word.text.lower() in [\"naar\", \"over\"]\n",
    "    ]\n",
    "\n",
    "    if len(prep_specifier_subtree_1) > 0:\n",
    "        prep_specifier_1 = [word.text.lower() for word in prep_specifier_subtree_1[0]]\n",
    "    else:\n",
    "        prep_specifier_1 = None\n",
    "    if len(prep_specifier_subtree_2) > 0:\n",
    "        prep_specifier_2 = [word.text.lower() for word in prep_specifier_subtree_2[0]]\n",
    "    else:\n",
    "        prep_specifier_2 = None\n",
    "\n",
    "    if prep_specifier_1 != prep_specifier_2:\n",
    "        return False\n",
    "\n",
    "    # If no contradictions to the conditions are found we return True\n",
    "    return True\n",
    "\n",
    "\n",
    "def event_contains_forward_links(event: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if an event contains forward links based on a stop word list.\n",
    "\n",
    "    Args:\n",
    "        event (Dict[str, Any]): The event for which we have to check the possible\n",
    "            link.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the event action contains a forward link and false otherwise.\n",
    "    \"\"\"\n",
    "    relevant_words = event[\"action\"][\"relevant_words\"]\n",
    "\n",
    "    forward_link_words = [\"slot\", \"slotte\", \"eindelijk\", \"uiteindelijk\"]\n",
    "\n",
    "    for word in relevant_words:\n",
    "        if word in forward_link_words:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def event_contains_loop_links(event: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Check if an event contains loop links based on a stop word list.\n",
    "\n",
    "    Args:\n",
    "        event (Dict[str, Any]): The event for which we have to check the possible\n",
    "            link.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the event action contains a loop link and False otherwise.\n",
    "    \"\"\"\n",
    "    relevant_words = event[\"action\"][\"relevant_words\"]\n",
    "\n",
    "    loop_link_words = [\n",
    "        \"volgende\",\n",
    "        \"vervolgens\",\n",
    "        \"daarna\",\n",
    "        \"nadien\",\n",
    "        \"hierna\",\n",
    "        \"verder\",\n",
    "        \"weer\",\n",
    "        \"opnieuw\",\n",
    "        \"nogmaals\",\n",
    "        \"wederom\",\n",
    "        \"terug\",\n",
    "        \"terugkeren\",\n",
    "    ]\n",
    "\n",
    "    for word in relevant_words:\n",
    "        if word in loop_link_words:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def determine_link_type(\n",
    "    source_event: Dict[str, Any],\n",
    "    target_event: Dict[str, Any],\n",
    "    sentence_information: List[Dict[str, Any]],\n",
    ") -> str:\n",
    "    \"\"\"[Algorithm 20] This function determines the type of link between two\n",
    "    events.\n",
    "\n",
    "    Args:\n",
    "        source_event (Dict[str, Any]): The event that is the source of the link\n",
    "        target_event (Dict[str, Any]): The event that is the target of the link\n",
    "        sentence_information (List[Dict[str, Any]]): A list with information on\n",
    "            all sentences in the document.\n",
    "    Returns:\n",
    "        str: The type of link (\"forward\", \"jump\", \"loop\" or None)\n",
    "    \"\"\"\n",
    "    conditional_indicators = [\n",
    "        \"als\",\n",
    "        \"hetzij\",\n",
    "        \"of\",\n",
    "        \"indien\",\n",
    "        \"geval\",\n",
    "        \"anders\",\n",
    "        \"optioneel\",\n",
    "        \"wanneer\",\n",
    "        \"als c\",\n",
    "    ]\n",
    "    if len(source_event[\"action\"][\"markers\"]) > 0:\n",
    "        # if we find conditional markers we check for forward and jump links.\n",
    "        if set(source_event[\"action\"][\"markers\"]).intersection(conditional_indicators):\n",
    "            if event_contains_forward_links(source_event):\n",
    "                return \"forward\"\n",
    "\n",
    "            target_sentence_id = target_event[\"action\"][\"sentence_id\"]\n",
    "            target_sentence = sentence_information[target_sentence_id]\n",
    "            target_tree = target_sentence[\"sentence_tree\"]\n",
    "\n",
    "            dependencies = find_dependency(\"conj\", target_tree, all=True)\n",
    "\n",
    "            # If we find a conjunction relationship where the conjunction is\n",
    "            # caused by an or-connection (the coordinating conjunction (cc) if\n",
    "            # 'of') we have a jump loop\n",
    "            for dependency in dependencies:\n",
    "                for element in [\"actor\", \"object\"]:\n",
    "                    if target_event[element]:\n",
    "                        target_element = target_event.get(element, {}).get(\n",
    "                            \"object\", [None]\n",
    "                        )[0]\n",
    "                        # Check ik the conjunction is with an element of this\n",
    "                        # event.\n",
    "                        if target_element == dependency.head:\n",
    "                            for event in target_sentence[\"events\"]:\n",
    "                                if event[element]:\n",
    "                                    # find the event with the conjunction we found\n",
    "                                    conjunct_element = event.get(element, {}).get(\n",
    "                                        \"object\", [None]\n",
    "                                    )[0]\n",
    "                                    if conjunct_element == dependency:\n",
    "                                        # check if the conjunction is caused by\n",
    "                                        # an if-relationship.\n",
    "                                        ccs = find_dependency(\n",
    "                                            \"cc\", target_tree, all=True\n",
    "                                        )\n",
    "                                        for cc in ccs:\n",
    "                                            if (\n",
    "                                                cc.head\n",
    "                                                in [target_element, conjunct_element]\n",
    "                                            ) and (cc.text.lower() == \"of\"):\n",
    "                                                return \"jump\"\n",
    "        else:\n",
    "            if event_contains_loop_links(source_event):\n",
    "                return \"loop\"\n",
    "    else:\n",
    "        if event_contains_loop_links(source_event):\n",
    "            return \"loop\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def determine_inter_action_links(\n",
    "    sentence_information: List[Dict[str, Any]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"[Algorithm 19] Identify links between actions\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): A list with information on\n",
    "            all sentences in the document.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The updated sentence information.\n",
    "    \"\"\"\n",
    "    events = [\n",
    "        event\n",
    "        for sentence in sentence_information\n",
    "        for event in sentence[\"events\"]\n",
    "        if event[\"action\"]\n",
    "    ]\n",
    "\n",
    "    # Go over all event combinations and link the two events together if they\n",
    "    # are linkable.\n",
    "    for event_1, event_2 in combinations(events, 2):\n",
    "        if is_linkable(event_1, event_2):\n",
    "            event_1_sentence_id = event_1[\"action\"][\"sentence_id\"]\n",
    "            event_1_index = sentence_information[event_1_sentence_id][\"events\"].index(\n",
    "                event_1\n",
    "            )\n",
    "            event_1[\"link\"] = event_2\n",
    "            link_type = determine_link_type(event_1, event_2, sentence_information)\n",
    "            event_1[\"link_type\"] = link_type\n",
    "            sentence_information[event_1_sentence_id][\"events\"][event_1_index] = event_1\n",
    "\n",
    "    return sentence_information\n",
    "\n",
    "\n",
    "def determine_conjoined_elements(\n",
    "    sentence: Dict[str, Any], event: Dict[str, Any], ignore_events: List[Dict[str, Any]]\n",
    ") -> List[Tuple[Dict[str, Any], str]]:\n",
    "    \"\"\"This function finds all events that have a conjunct element with the\n",
    "    main event.\n",
    "\n",
    "    Args:\n",
    "        sentence (Dict[str, Any]): The information for the sentence containing\n",
    "            the event.\n",
    "        event (Dict[str, Any]): The event for which to find conjunctions.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Dict[str, Any], str]]: A list of conjunct events.\n",
    "    \"\"\"\n",
    "    conjoined_result = []\n",
    "    dependencies = find_dependency(\"conj\", sentence[\"sentence_tree\"], all=True)\n",
    "\n",
    "    ignore_events.append(event)\n",
    "\n",
    "    for dependency in dependencies:\n",
    "        # check for conjunctions for each element\n",
    "        for element in [\"action\", \"actor\", \"object\"]:\n",
    "            if event[element]:\n",
    "                target_element = event[element][\"object\"][0]\n",
    "                if (target_element == dependency.head) or (\n",
    "                    target_element == dependency\n",
    "                ):\n",
    "                    # Find the conjunct events by looking at all events that\n",
    "                    # have the element we need.\n",
    "                    for conjunct_event in [\n",
    "                        sentence_event\n",
    "                        for sentence_event in sentence[\"events\"]\n",
    "                        if sentence_event[element]\n",
    "                    ]:\n",
    "                        if conjunct_event not in ignore_events:\n",
    "                            conjunct_element = conjunct_event.get(element, {}).get(\n",
    "                                \"object\", [None]\n",
    "                            )[0]\n",
    "                            # If the event is conjunct to the main event we find\n",
    "                            # the coordinating conjunction and store a combination\n",
    "                            # of the two.\n",
    "                            if (conjunct_element == dependency) or (\n",
    "                                conjunct_element == dependency.head\n",
    "                            ):\n",
    "                                ccs = find_dependency(\n",
    "                                    \"cc\", sentence[\"sentence_tree\"], all=True\n",
    "                                )\n",
    "                                for cc in ccs:\n",
    "                                    # Some words are seen as coordinating\n",
    "                                    # conjunctions while they should not be\n",
    "                                    # these are stored here to be skipped.\n",
    "                                    if cc.text.lower() in [\"a\", \"doch\", \"maar\", \"e.\"]:\n",
    "                                        continue\n",
    "                                    if cc.head in [target_element, conjunct_element]:\n",
    "                                        # We keep track of all event that have\n",
    "                                        # already been added to prevent infinite\n",
    "                                        # loops.\n",
    "                                        ignore_events.append(conjunct_event)\n",
    "                                        conjoined_result.append(\n",
    "                                            (conjunct_event, element, cc)\n",
    "                                        )\n",
    "                                        conjoined_result += (\n",
    "                                            determine_conjoined_elements(\n",
    "                                                sentence, conjunct_event, ignore_events\n",
    "                                            )\n",
    "                                        )\n",
    "\n",
    "    return conjoined_result\n",
    "\n",
    "\n",
    "def build_gateway(\n",
    "    last_actions: List[Dict[str, Any]],\n",
    "    open_split: List[Dict[str, Any]],\n",
    "    conjoined_result: List[Tuple[Dict[str, Any], str]],\n",
    "    event: Dict[str, Any],\n",
    "    flow: List[Dict[str, Any]],\n",
    "    type: str,\n",
    ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"This function builds a split in the flow where two events will follow\n",
    "    from the previous one.\n",
    "\n",
    "    Args:\n",
    "        last_actions (List[Dict[str, Any]]): All previous actions\n",
    "        open_split (List[Dict[str, Any]]): The last actions in an open split\n",
    "        conjoined_result (List[Tuple[Dict[str, Any], str]]): The event\n",
    "            following the split\n",
    "        event (Dict[str, Any]): Another event following the split\n",
    "        flow (List[Dict[str, Any]]): All flow elements found so far\n",
    "        type (str): The type of split being created (\"of\" or \"en\")\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "            The updated, flow, last_actions, and open_split lists.\n",
    "    \"\"\"\n",
    "    # If there is a previous event this is our starting point\n",
    "    if len(last_actions) > 0:\n",
    "        from_event = last_actions[-1]\n",
    "    else:\n",
    "        from_event = None\n",
    "\n",
    "    flow_element = {\n",
    "        \"from_event\": from_event,\n",
    "        \"to_event\": event,\n",
    "        \"type\": type,\n",
    "        \"split\": True,\n",
    "    }\n",
    "    flow.append(flow_element)\n",
    "    last_actions.append(event)\n",
    "    open_split.append(event)\n",
    "\n",
    "    # make a flow from the starting point to each of the events that are a part\n",
    "    # of this split and add the type of the split tot the flows.\n",
    "    for conjoined in conjoined_result:\n",
    "        flow_element = {\n",
    "            \"from_event\": from_event,\n",
    "            \"to_event\": conjoined[0],\n",
    "            \"type\": type,\n",
    "            \"split\": True,\n",
    "        }\n",
    "        flow.append(flow_element)\n",
    "        last_actions.append(conjoined[0])\n",
    "        # Add the events to the open_split\n",
    "        open_split.append(conjoined[0])\n",
    "\n",
    "    return flow, last_actions, open_split\n",
    "\n",
    "\n",
    "def build_join(\n",
    "    flow: List[Dict[str, Any]],\n",
    "    last_actions: List[Dict[str, Any]],\n",
    "    open_split: List[Dict[str, Any]],\n",
    "    dummy_id: int,\n",
    ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"This functions joins the current split into a dummy node.\n",
    "\n",
    "    Args:\n",
    "        flow (List[Dict[str, Any]]): A list of all flow elements found so far.\n",
    "        last_actions (List[Dict[str, Any]]): A list of previous events.\n",
    "        open_split (List[Dict[str, Any]]): A list of event in the open split.\n",
    "        dummy_id (int): ID for the dummy to distinguish them later.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "            The updated flow, last_actions and open_split lists.\n",
    "    \"\"\"\n",
    "    # For each event in the open split create a flow to the same dummy join node.\n",
    "    for split_event in open_split:\n",
    "        new_flow_element = {\n",
    "            \"from_event\": split_event,\n",
    "            \"to_event\": {\"object\": f\"Dummy Node join {dummy_id}\"},\n",
    "            \"type\": \"join\",\n",
    "            \"split\": False,\n",
    "        }\n",
    "        flow.append(new_flow_element)\n",
    "\n",
    "    last_actions.append({\"object\": f\"Dummy Node join {dummy_id}\"})\n",
    "\n",
    "    open_split = []\n",
    "\n",
    "    return flow, last_actions, open_split\n",
    "\n",
    "\n",
    "def handle_single_action(\n",
    "    flow: List[Dict[str, Any]],\n",
    "    event: Dict[str, Any],\n",
    "    last_actions: List[Dict[str, Any]],\n",
    "    open_split: List[Dict[str, Any]],\n",
    "    dummy_id: int,\n",
    ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]], int]:\n",
    "    \"\"\"[Algorithm 22] Add a single action to the flow.\n",
    "\n",
    "    Args:\n",
    "        flow (List[Dict[str, Any]]): A list of all flow elements found so far.\n",
    "        event (Dict[str, Any]): The event to add.\n",
    "        last_actions (List[Dict[str, Any]]): A list of previous events.\n",
    "        open_split (List[Dict[str, Any]]): A list of event in the open split.\n",
    "        dummy_id (int): ID for the dummy to distinguish them later.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "            The updated flow, last_actions, open_split lists, and dummy_id.\n",
    "    \"\"\"\n",
    "    # Define the stop word lists.\n",
    "    conditional_indicators = [\n",
    "        \"als\",\n",
    "        \"hetzij\",\n",
    "        \"of\",\n",
    "        \"indien\",\n",
    "        \"geval\",\n",
    "        \"anders\",\n",
    "        \"optioneel\",\n",
    "        \"wanneer\",\n",
    "        \"als c\",\n",
    "    ]\n",
    "\n",
    "    parallel_indicators = [\n",
    "        \"terwijl\",\n",
    "        \"tussentijd\",\n",
    "        \"parallel\",\n",
    "        \"gelijktijdig\",\n",
    "        \"ondertussen\",\n",
    "        \"tegelijkertijd\",\n",
    "        \"terwijl c\",\n",
    "    ]\n",
    "\n",
    "    sequence_indicators = [\n",
    "        \"dan\",\n",
    "        \"na\",\n",
    "        \"nadien\",\n",
    "        \"daarna\",\n",
    "        \"vervolgens\",\n",
    "        \"basis\",\n",
    "        \"gebaseerd\",\n",
    "        \"dus\",\n",
    "        \"dan c\",\n",
    "    ]\n",
    "    # if we find sequence_indicators or conditional_indicators we close the\n",
    "    # current split.\n",
    "    if event[\"action\"][\"markers\"] != []:\n",
    "        if (set(event[\"action\"][\"markers\"]).intersection(sequence_indicators)) or (\n",
    "            set(event[\"action\"][\"markers\"]).intersection(conditional_indicators)\n",
    "        ):\n",
    "            if open_split:\n",
    "                flow, last_actions, open_split = build_join(\n",
    "                    flow, last_actions, open_split, dummy_id\n",
    "                )\n",
    "                dummy_id += 1\n",
    "\n",
    "    if event[\"action\"][\"markers\"] != []:\n",
    "        # if we find parallel indicators we create an and-split by putting this\n",
    "        # event parallel to the last event added to the flowchart.\n",
    "        if set(event[\"action\"][\"markers\"]).intersection(parallel_indicators):\n",
    "            if last_actions:\n",
    "                flow_from = flow[-1][\"from_event\"]\n",
    "                flow_to = flow[-1][\"to_event\"]\n",
    "            else:\n",
    "                flow_element = {\n",
    "                    \"from_event\": None,\n",
    "                    \"to_event\": {\"object\": f\"Dummy Node split {dummy_id}\"},\n",
    "                    \"type\": \"en\",\n",
    "                    \"split\": False,\n",
    "                }\n",
    "                flow.append(flow_element)\n",
    "                flow_from = {\"object\": f\"Dummy Node split {dummy_id}\"}\n",
    "                flow_to = None\n",
    "                dummy_id += 1\n",
    "\n",
    "            new_flow = {\n",
    "                \"from_event\": flow_from,\n",
    "                \"to_event\": event,\n",
    "                \"type\": \"en\",\n",
    "                \"split\": True,\n",
    "            }\n",
    "            flow[-1][\"type\"] = \"en\"\n",
    "            flow[-1][\"split\"] = True\n",
    "            open_split = [flow_to, event] if flow_to else [event]\n",
    "            flow.append(new_flow)\n",
    "            last_actions.append(event)\n",
    "\n",
    "        else:\n",
    "            # otherwise we add the event sequentially. Either behind a start\n",
    "            # node if it is the first event or behind the previous node.\n",
    "            if len(last_actions) > 0:\n",
    "                new_flow = {\n",
    "                    \"from_event\": last_actions[-1],\n",
    "                    \"to_event\": event,\n",
    "                    \"type\": None,\n",
    "                    \"split\": False,\n",
    "                }\n",
    "                flow.append(new_flow)\n",
    "                if last_actions[-1] in open_split:\n",
    "                    replace_index = open_split.index(last_actions[-1])\n",
    "                    open_split[replace_index] = event\n",
    "                last_actions.append(event)\n",
    "            else:\n",
    "                new_flow = {\n",
    "                    \"from_event\": \"start\",\n",
    "                    \"to_event\": event,\n",
    "                    \"type\": None,\n",
    "                    \"split\": False,\n",
    "                }\n",
    "                flow.append(new_flow)\n",
    "                last_actions.append(event)\n",
    "    else:\n",
    "        if len(last_actions) > 0:\n",
    "            new_flow = {\n",
    "                \"from_event\": last_actions[-1],\n",
    "                \"to_event\": event,\n",
    "                \"type\": None,\n",
    "                \"split\": False,\n",
    "            }\n",
    "            flow.append(new_flow)\n",
    "            if last_actions[-1] in open_split:\n",
    "                replace_index = open_split.index(last_actions[-1])\n",
    "                open_split[replace_index] = event\n",
    "            last_actions.append(event)\n",
    "        else:\n",
    "            new_flow = {\n",
    "                \"from_event\": \"start\",\n",
    "                \"to_event\": event,\n",
    "                \"type\": None,\n",
    "                \"split\": False,\n",
    "            }\n",
    "            flow.append(new_flow)\n",
    "            last_actions.append(event)\n",
    "\n",
    "    return flow, last_actions, open_split, dummy_id\n",
    "\n",
    "\n",
    "def build_flows(sentence_information: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"[Algorithm 21] This function identifies the flow between all events in a\n",
    "    document.\n",
    "\n",
    "    Args:\n",
    "        sentence_information (List[Dict[str, Any]]): A list with information on\n",
    "            all sentences in the document.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of all Flows that will end up in the\n",
    "            flowchart.\n",
    "    \"\"\"\n",
    "    # 'dan' is included as the construction 'dan wel' is an or indicator\n",
    "    or_indicators = [\"of\", \"hetzij\", \"noch\", \"respectievelijk\", \"dan\"]\n",
    "    and_indicators = [\"en\", \"alsmede\", \"onderscheidenlijk\", \"zowel\", \"tot\"]\n",
    "\n",
    "    last_actions = []\n",
    "    open_split = []\n",
    "    flow = []\n",
    "    processed_events = []\n",
    "    dummy_id = 0\n",
    "    for i, sentence in enumerate(sentence_information):\n",
    "        for j, event in enumerate(sentence[\"events\"]):\n",
    "            if event not in processed_events:\n",
    "                # prevent events being processed twice.\n",
    "                processed_events.append(event)\n",
    "                if event[\"action\"]:\n",
    "                    # if we encounter a jump link we start a new part of the flow.\n",
    "                    if event[\"link_type\"] == \"jump\":\n",
    "                        last_actions = []\n",
    "                        open_split = []\n",
    "                        new_flow = {\n",
    "                            \"from_event\": None,\n",
    "                            \"to_event\": event[\"link\"],\n",
    "                            \"type\": None,\n",
    "                            \"split\": False,\n",
    "                        }\n",
    "                        flow.append(new_flow)\n",
    "                        last_actions.append(event[\"link\"])\n",
    "\n",
    "                    # Find all conjunct events and the corresponding coordinating\n",
    "                    # conjunctions.\n",
    "                    conjoined_result = determine_conjoined_elements(sentence, event, [])\n",
    "                    if len(conjoined_result) == 0:\n",
    "                        # if no conjunctions are found, handle single action\n",
    "                        flow, last_actions, open_split, dummy_id = handle_single_action(\n",
    "                            flow, event, last_actions, open_split, dummy_id\n",
    "                        )\n",
    "                    else:\n",
    "                        coordinating_conjunctions = [\n",
    "                            conjoined_element[2].text.lower()\n",
    "                            for conjoined_element in conjoined_result\n",
    "                        ]\n",
    "\n",
    "                        # If there are any coordinating conjunctions that are\n",
    "                        # not either in the or_indicators or and_indicators we\n",
    "                        # print this coordinating conjunction in an error\n",
    "                        # message so it can be added to one of the lists or\n",
    "                        # excluded as coordinating conjunction.\n",
    "                        if any(\n",
    "                            (coordinating_conj not in or_indicators)\n",
    "                            and (coordinating_conj not in and_indicators)\n",
    "                            for coordinating_conj in coordinating_conjunctions\n",
    "                        ):\n",
    "                            missing_words = [\n",
    "                                coordinating_conj\n",
    "                                for coordinating_conj in coordinating_conjunctions\n",
    "                                if (coordinating_conj not in or_indicators)\n",
    "                                and (coordinating_conj not in and_indicators)\n",
    "                            ]\n",
    "                            raise ValueError(\n",
    "                                f\"Connection word(s) {missing_words} has not been added to a list yet, please add it and re-run the code. Sentence: {sentence['sentence']}\"\n",
    "                            )\n",
    "\n",
    "                        # If we only have or_indicators we make a or-split\n",
    "                        # ('of' in Dutch) of all conjoined events.\n",
    "                        if all(\n",
    "                            coordinating_conj in or_indicators\n",
    "                            for coordinating_conj in coordinating_conjunctions\n",
    "                        ):\n",
    "                            if last_actions != []:\n",
    "                                dummy_flow = {\n",
    "                                    \"from_event\": last_actions[-1],\n",
    "                                    \"to_event\": {\n",
    "                                        \"object\": f\"Dummy Node split {dummy_id}\"\n",
    "                                    },\n",
    "                                    \"type\": \"of\",\n",
    "                                    \"split\": False,\n",
    "                                }\n",
    "                            else:\n",
    "                                dummy_flow = {\n",
    "                                    \"from_event\": \"start\",\n",
    "                                    \"to_event\": {\n",
    "                                        \"object\": f\"Dummy Node split {dummy_id}\"\n",
    "                                    },\n",
    "                                    \"type\": \"of\",\n",
    "                                    \"split\": False,\n",
    "                                }\n",
    "                            flow.append(dummy_flow)\n",
    "                            last_actions.append(\n",
    "                                {\"object\": f\"Dummy Node split {dummy_id}\"}\n",
    "                            )\n",
    "                            dummy_id += 1\n",
    "                            flow, last_actions, open_split = build_gateway(\n",
    "                                last_actions,\n",
    "                                open_split,\n",
    "                                conjoined_result,\n",
    "                                event,\n",
    "                                flow,\n",
    "                                \"of\",\n",
    "                            )\n",
    "                            # we immediately join the split\n",
    "                            flow, last_actions, open_split = build_join(\n",
    "                                flow,\n",
    "                                last_actions,\n",
    "                                open_split,\n",
    "                                dummy_id,\n",
    "                            )\n",
    "                            dummy_id += 1\n",
    "                            processed_events += [\n",
    "                                conjoined[0] for conjoined in conjoined_result\n",
    "                            ]\n",
    "\n",
    "                        elif all(\n",
    "                            coordinating_conj in and_indicators\n",
    "                            for coordinating_conj in coordinating_conjunctions\n",
    "                        ):\n",
    "                            # If we only have and_indicators and the split is\n",
    "                            # based on the actors we make a and-split ('en' in\n",
    "                            # Dutch) of all conjoined events.\n",
    "                            if all(\n",
    "                                conjoined[1] == \"actor\"\n",
    "                                for conjoined in conjoined_result\n",
    "                            ):\n",
    "                                if last_actions != []:\n",
    "                                    dummy_flow = {\n",
    "                                        \"from_event\": last_actions[-1],\n",
    "                                        \"to_event\": {\n",
    "                                            \"object\": f\"Dummy Node split {dummy_id}\"\n",
    "                                        },\n",
    "                                        \"type\": \"en\",\n",
    "                                        \"split\": False,\n",
    "                                    }\n",
    "                                else:\n",
    "                                    dummy_flow = {\n",
    "                                        \"from_event\": \"start\",\n",
    "                                        \"to_event\": {\n",
    "                                            \"object\": f\"Dummy Node split {dummy_id}\"\n",
    "                                        },\n",
    "                                        \"type\": \"en\",\n",
    "                                        \"split\": False,\n",
    "                                    }\n",
    "                                flow.append(dummy_flow)\n",
    "                                last_actions.append(\n",
    "                                    {\"object\": f\"Dummy Node split {dummy_id}\"}\n",
    "                                )\n",
    "                                dummy_id += 1\n",
    "                                flow, last_actions, open_split = build_gateway(\n",
    "                                    last_actions,\n",
    "                                    open_split,\n",
    "                                    conjoined_result,\n",
    "                                    event,\n",
    "                                    flow,\n",
    "                                    \"en\",\n",
    "                                )\n",
    "                                # we immediately join the split\n",
    "                                flow, last_actions, open_split = build_join(\n",
    "                                    flow,\n",
    "                                    last_actions,\n",
    "                                    open_split,\n",
    "                                    dummy_id,\n",
    "                                )\n",
    "                                dummy_id += 1\n",
    "                                processed_events += [\n",
    "                                    conjoined[0] for conjoined in conjoined_result\n",
    "                                ]\n",
    "                            # if we only have and_indicators but the conjunction\n",
    "                            # is not based on the actor we simply close any open\n",
    "                            # split and handle single action.\n",
    "                            else:\n",
    "                                if open_split:\n",
    "                                    flow, last_actions, open_split = build_join(\n",
    "                                        flow,\n",
    "                                        last_actions,\n",
    "                                        open_split,\n",
    "                                        dummy_id,\n",
    "                                    )\n",
    "                                    dummy_id += 1\n",
    "                                flow, last_actions, open_split, dummy_id = (\n",
    "                                    handle_single_action(\n",
    "                                        flow, event, last_actions, open_split, dummy_id\n",
    "                                    )\n",
    "                                )\n",
    "                        # If there are both or_indicators and and_indicators\n",
    "                        # we make an en/of split. Indicating any number of\n",
    "                        # paths can be taken.\n",
    "                        else:\n",
    "                            if last_actions != []:\n",
    "                                dummy_flow = {\n",
    "                                    \"from_event\": last_actions[-1],\n",
    "                                    \"to_event\": {\n",
    "                                        \"object\": f\"Dummy Node split {dummy_id}\"\n",
    "                                    },\n",
    "                                    \"type\": \"en/of\",\n",
    "                                    \"split\": False,\n",
    "                                }\n",
    "                            else:\n",
    "                                dummy_flow = {\n",
    "                                    \"from_event\": \"start\",\n",
    "                                    \"to_event\": {\n",
    "                                        \"object\": f\"Dummy Node split {dummy_id}\"\n",
    "                                    },\n",
    "                                    \"type\": \"en/of\",\n",
    "                                    \"split\": False,\n",
    "                                }\n",
    "                            flow.append(dummy_flow)\n",
    "                            last_actions.append(\n",
    "                                {\"object\": f\"Dummy Node split {dummy_id}\"}\n",
    "                            )\n",
    "                            dummy_id += 1\n",
    "                            flow, last_actions, open_split = build_gateway(\n",
    "                                last_actions,\n",
    "                                open_split,\n",
    "                                conjoined_result,\n",
    "                                event,\n",
    "                                flow,\n",
    "                                \"en/of\",\n",
    "                            )\n",
    "                            # We immediately join the split\n",
    "                            flow, last_actions, open_split = build_join(\n",
    "                                flow,\n",
    "                                last_actions,\n",
    "                                open_split,\n",
    "                                dummy_id,\n",
    "                            )\n",
    "                            dummy_id += 1\n",
    "                            processed_events += [\n",
    "                                conjoined[0] for conjoined in conjoined_result\n",
    "                            ]\n",
    "    return flow\n",
    "\n",
    "\n",
    "def generate_chat_gpt_output(flows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate the flowchart summarization for ChatGPT to use to improve.\n",
    "\n",
    "    Args:\n",
    "        flows (List[Dict[str, Any]]): The flows between events\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str,Any]]: The flows ChatGPT should improve.\n",
    "    \"\"\"\n",
    "    final_output = []\n",
    "    for flow in flows:\n",
    "        from_event_text = None\n",
    "        to_event_text = None\n",
    "        object_reference = None\n",
    "        actor_reference = None\n",
    "        # For both the from event and the to event determine the text that\n",
    "        # would be written in the corresponding node of the flowchart. We\n",
    "        # exclude marks from the event text as this makes more sense when\n",
    "        # printing.\n",
    "        if flow[\"from_event\"] == \"start\":\n",
    "            from_event_text = \"start\"\n",
    "        elif flow[\"from_event\"] is None:\n",
    "            from_event_text = \" \"\n",
    "        elif flow[\"from_event\"][\"object\"]:\n",
    "            if \"Dummy Node split\" in flow[\"from_event\"][\"object\"]:\n",
    "                from_event_text = flow[\"type\"]\n",
    "            elif \"Dummy Node join\" in flow[\"from_event\"][\"object\"]:\n",
    "                from_event_text = \"join\"\n",
    "        if not from_event_text:\n",
    "            # get the relevant text for the object\n",
    "            if flow[\"from_event\"][\"object\"]:\n",
    "                if flow[\"from_event\"][\"object\"][\"reference\"]:\n",
    "                    object_reference = flow[\"from_event\"][\"object\"][\"reference\"][\"text\"]\n",
    "                object_text = [\n",
    "                    word\n",
    "                    for word in flow[\"from_event\"][\"object\"][\"relevant_words\"]\n",
    "                    if word.dep_ != \"mark\"\n",
    "                ]\n",
    "            else:\n",
    "                object_text = []\n",
    "\n",
    "            # get the relevant text for the actor\n",
    "            if flow[\"from_event\"][\"actor\"]:\n",
    "                if flow[\"from_event\"][\"actor\"][\"reference\"]:\n",
    "                    actor_reference = flow[\"from_event\"][\"actor\"][\"reference\"][\"text\"]\n",
    "                actor_text = [\n",
    "                    word\n",
    "                    for word in flow[\"from_event\"][\"actor\"][\"relevant_words\"]\n",
    "                    if word.dep_ != \"mark\"\n",
    "                ]\n",
    "            else:\n",
    "                actor_text = []\n",
    "\n",
    "            # get the relevant text for the action\n",
    "            action_text = [\n",
    "                word\n",
    "                for word in flow[\"from_event\"][\"action\"][\"relevant_words\"]\n",
    "                if word.dep_ != \"mark\"\n",
    "            ]\n",
    "\n",
    "            all_relevant_words = list(\n",
    "                set(list(object_text) + list(actor_text) + list(action_text))\n",
    "            )\n",
    "            # make sure all words are in the same order as they would be in the\n",
    "            # original text.\n",
    "            all_relevant_words.sort(key=lambda word: word.i)\n",
    "\n",
    "            # replace the object and actor with their reference in the text\n",
    "            if object_reference:\n",
    "                all_relevant_words[all_relevant_words.index(list(object_text)[0])] = (\n",
    "                    object_reference\n",
    "                )\n",
    "            if actor_reference:\n",
    "                all_relevant_words[all_relevant_words.index(list(actor_text)[0])] = (\n",
    "                    actor_reference\n",
    "                )\n",
    "\n",
    "            all_relevant_words = [\n",
    "                word.text.lower() if not isinstance(word, str) else word\n",
    "                for word in all_relevant_words\n",
    "            ]\n",
    "            from_event_text = \" \".join(all_relevant_words)\n",
    "\n",
    "        object_reference = None\n",
    "        actor_reference = None\n",
    "\n",
    "        # Repeat the exact same process for the to event\n",
    "        if flow[\"to_event\"][\"object\"]:\n",
    "            if \"Dummy Node split\" in flow[\"to_event\"][\"object\"]:\n",
    "                to_event_text = flow[\"type\"]\n",
    "            elif \"Dummy Node join\" in flow[\"to_event\"][\"object\"]:\n",
    "                to_event_text = \"join\"\n",
    "        if not to_event_text:\n",
    "            if flow[\"to_event\"][\"object\"]:\n",
    "                if flow[\"to_event\"][\"object\"][\"reference\"]:\n",
    "                    object_reference = flow[\"to_event\"][\"object\"][\"reference\"][\"text\"]\n",
    "                object_text = [\n",
    "                    word\n",
    "                    for word in flow[\"to_event\"][\"object\"][\"relevant_words\"]\n",
    "                    if word.dep_ != \"mark\"\n",
    "                ]\n",
    "            else:\n",
    "                object_text = []\n",
    "\n",
    "            if flow[\"to_event\"][\"actor\"]:\n",
    "                if flow[\"to_event\"][\"actor\"][\"reference\"]:\n",
    "                    actor_reference = flow[\"to_event\"][\"actor\"][\"reference\"][\"text\"]\n",
    "                actor_text = [\n",
    "                    word\n",
    "                    for word in flow[\"to_event\"][\"actor\"][\"relevant_words\"]\n",
    "                    if word.dep_ != \"mark\"\n",
    "                ]\n",
    "            else:\n",
    "                actor_text = []\n",
    "\n",
    "            action_text = [\n",
    "                word\n",
    "                for word in flow[\"to_event\"][\"action\"][\"relevant_words\"]\n",
    "                if word.dep_ != \"mark\"\n",
    "            ]\n",
    "\n",
    "            all_relevant_words = list(\n",
    "                set(list(object_text) + list(actor_text) + list(action_text))\n",
    "            )\n",
    "            all_relevant_words.sort(key=lambda word: word.i)\n",
    "\n",
    "            if object_reference:\n",
    "                all_relevant_words[all_relevant_words.index(list(object_text)[0])] = (\n",
    "                    object_reference\n",
    "                )\n",
    "            if actor_reference:\n",
    "                all_relevant_words[all_relevant_words.index(list(actor_text)[0])] = (\n",
    "                    actor_reference\n",
    "                )\n",
    "\n",
    "            all_relevant_words = [\n",
    "                word.text.lower() if not isinstance(word, str) else word\n",
    "                for word in all_relevant_words\n",
    "            ]\n",
    "            to_event_text = \" \".join(all_relevant_words)\n",
    "        # define the relevant information for the flow\n",
    "        new_flow = {\"from_event\": from_event_text, \"to_event\": to_event_text}\n",
    "        final_output.append(new_flow)\n",
    "\n",
    "    print(final_output)\n",
    "    return final_output\n",
    "\n",
    "\n",
    "def create_node_from_event(\n",
    "    event: Dict[str, Any], flow: Dict[str, Any], node_id: int, graph: graphviz.Digraph\n",
    ") -> graphviz.Digraph:\n",
    "    \"\"\"Create a node to put into the flowchart by extracting the relevant\n",
    "    information from the event and the flow.\n",
    "\n",
    "    Args:\n",
    "        event (Dict[str, Any]): The event for which we create a node.\n",
    "        flow (Dict[str, Any]): The flow showing the split type if we create a\n",
    "            dummy node.\n",
    "        node_id (int): The id of the node in the graph.\n",
    "        graph (graphviz.Digraph): The flowchart.\n",
    "\n",
    "    Returns:\n",
    "        graphviz.Digraph: The flowchart with a node added.\n",
    "    \"\"\"\n",
    "    object_reference = None\n",
    "    actor_reference = None\n",
    "\n",
    "    # get the event description\n",
    "    if event[\"object\"]:\n",
    "        if \"Dummy Node split\" in event[\"object\"]:\n",
    "            graph.node(str(node_id), flow[\"type\"])\n",
    "            return graph\n",
    "        elif \"Dummy Node join\" in event[\"object\"]:\n",
    "            graph.node(str(node_id), \"Join\")\n",
    "            return graph\n",
    "\n",
    "    # get all relevant words for the object\n",
    "    if event[\"object\"]:\n",
    "        if event[\"object\"][\"reference\"]:\n",
    "            object_reference = event[\"object\"][\"reference\"][\"text\"]\n",
    "        object_text = [\n",
    "            word for word in event[\"object\"][\"relevant_words\"] if word.dep_ != \"mark\"\n",
    "        ]\n",
    "    else:\n",
    "        object_text = []\n",
    "\n",
    "    # get all relevant words for the actor\n",
    "    if event[\"actor\"]:\n",
    "        if event[\"actor\"][\"reference\"]:\n",
    "            actor_reference = event[\"actor\"][\"reference\"][\"text\"]\n",
    "        actor_text = [\n",
    "            word for word in event[\"actor\"][\"relevant_words\"] if word.dep_ != \"mark\"\n",
    "        ]\n",
    "    else:\n",
    "        actor_text = []\n",
    "\n",
    "    # get all relevant words for the action\n",
    "    action_text = [\n",
    "        word for word in event[\"action\"][\"relevant_words\"] if word.dep_ != \"mark\"\n",
    "    ]\n",
    "\n",
    "    all_relevant_words = list(\n",
    "        set(list(object_text) + list(actor_text) + list(action_text))\n",
    "    )\n",
    "    # make sure all words are in the same order as they are in the original text\n",
    "    all_relevant_words.sort(key=lambda word: word.i)\n",
    "\n",
    "    # replace the actor and object with their reference if they have one.\n",
    "    if object_reference:\n",
    "        all_relevant_words[all_relevant_words.index(list(object_text)[0])] = (\n",
    "            object_reference\n",
    "        )\n",
    "    if actor_reference:\n",
    "        all_relevant_words[all_relevant_words.index(list(actor_text)[0])] = (\n",
    "            actor_reference\n",
    "        )\n",
    "\n",
    "    all_relevant_words = [\n",
    "        word.text.lower() if not isinstance(word, str) else word\n",
    "        for word in all_relevant_words\n",
    "    ]\n",
    "    event_text = \" \".join(all_relevant_words)\n",
    "\n",
    "    graph.node(str(node_id), event_text)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_flow_diagram(\n",
    "    flow: List[Dict[str, Any]], article: str, output_path: str\n",
    ") -> None:\n",
    "    \"\"\"Create the flow diagram image from the flows.\n",
    "\n",
    "    Args:\n",
    "        flow (List[Dict[str, Any]]): THe flows that should be made into a\n",
    "            flowchart.\n",
    "        article (str): The name of the article the flowchart is for.\n",
    "        output_path (str): The path the flowchart should be saved to.\n",
    "    \"\"\"\n",
    "    graph = graphviz.Digraph(\"article_flow\", comment=\"Article of Interest\")\n",
    "    graph.node(\"start\", \"Start\")\n",
    "    event_node_labels = []\n",
    "    processed_events = []\n",
    "    for i, flow_element in enumerate(flow):\n",
    "        # If the node for the event already exists find the node, if the node\n",
    "        # for the event does not exist make the node.\n",
    "        to_event = flow_element[\"to_event\"]\n",
    "        if to_event in processed_events:\n",
    "            to_node_label = event_node_labels[processed_events.index(to_event)]\n",
    "        else:\n",
    "            graph = create_node_from_event(to_event, flow_element, i, graph)\n",
    "            to_node_label = str(i)\n",
    "            event_node_labels.append(str(i))\n",
    "            processed_events.append(to_event)\n",
    "\n",
    "        # Find the from node and create an edge between the from and to node.\n",
    "        if flow_element[\"from_event\"]:\n",
    "            if flow_element[\"from_event\"] == \"start\":\n",
    "                graph.edge(\"start\", to_node_label)\n",
    "            else:\n",
    "                from_event = flow_element[\"from_event\"]\n",
    "                from_node_label = event_node_labels[processed_events.index(from_event)]\n",
    "                graph.edge(from_node_label, to_node_label)\n",
    "\n",
    "    graph.render(\n",
    "        os.path.join(output_path, article.replace(\":\", \".\")), format=\"png\", view=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def generate_chat_gpt_output(\n",
    "    flows: List[Dict[str, Any]],\n",
    "    new_event_list: List[Dict[str, Any]],\n",
    "    original_event_list: List[Dict[str, Any]],\n",
    ") -> None:\n",
    "    \"\"\"Create the flowchart representation that ChatGPT needs to improve in \n",
    "    its final task using the corrected event descriptions created by ChatGPT.\n",
    "\n",
    "    Args:\n",
    "        flows (List[Dict[str, Any]]): All flows extracted by the algorithm.\n",
    "        new_event_list (List[Dict[str, Any]]): The corrected event list created \n",
    "            by ChatGPT.\n",
    "        original_event_list (List[Dict[str, Any]]): The original event list the \n",
    "            flows are based on.\n",
    "    \"\"\"\n",
    "    final_output = []\n",
    "    for flow in flows:\n",
    "        from_event_text = None\n",
    "        to_event_text = None\n",
    "        \n",
    "        # find text of the from event node\n",
    "        if flow[\"from_event\"] == \"start\":\n",
    "            from_event_text = \"start\"\n",
    "        elif flow[\"from_event\"] is None:\n",
    "            from_event_text = \" \"\n",
    "        elif flow[\"from_event\"][\"object\"]:\n",
    "            if \"Dummy Node split\" in flow[\"from_event\"][\"object\"]:\n",
    "                # include dummy id as ChatGPT should also include them\n",
    "                dummy_id = re.findall(r\"\\d+\", flow[\"from_event\"][\"object\"])\n",
    "                from_event_text = flow[\"type\"] + f\" {dummy_id[0]}\"\n",
    "            elif \"Dummy Node join\" in flow[\"from_event\"][\"object\"]:\n",
    "                dummy_id = re.findall(r\"\\d+\", flow[\"from_event\"][\"object\"])\n",
    "                from_event_text = f\"join {dummy_id[0]}\"\n",
    "        if not from_event_text:\n",
    "            # If the event is not start None or a Dummy node it should be in the original event list\n",
    "            from_event_id = original_event_list.index(flow[\"from_event\"])\n",
    "            # We find the corresponding corrected event description created by ChatGPT\n",
    "            from_event_text = [event for event in new_event_list if event[\"original_event_id\"] == (from_event_id+1)][0][\"event_description\"]\n",
    "\n",
    "        # Similarly find the to event text\n",
    "        if flow[\"to_event\"][\"object\"]:\n",
    "            if \"Dummy Node split\" in flow[\"to_event\"][\"object\"]:\n",
    "                dummy_id = re.findall(r\"\\d+\", flow[\"to_event\"][\"object\"])\n",
    "                to_event_text = flow[\"type\"] + f\" {dummy_id[0]}\"\n",
    "            elif \"Dummy Node join\" in flow[\"to_event\"][\"object\"]:\n",
    "                dummy_id = re.findall(r\"\\d+\", flow[\"to_event\"][\"object\"])\n",
    "                to_event_text = f\"join {dummy_id[0]}\"\n",
    "        if not to_event_text:\n",
    "            to_event_id = original_event_list.index(flow[\"to_event\"])\n",
    "            to_event_text = [event for event in new_event_list if event[\"original_event_id\"] == (to_event_id+1)][0][\"event_description\"]\n",
    "        \n",
    "        # Define the flow based on the text only.\n",
    "        new_flow = {\"from_event\": from_event_text, \"to_event\": to_event_text}\n",
    "        final_output.append(new_flow)\n",
    "\n",
    "    print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'event_description': 'de verdachte wordt opgehouden voor onderzoek', 'action': {'text': 'dat wordt opgehouden voor onderzoek'}, 'actor': None, 'object': {'text': 'de verdachte', 'reference': None}}, {'event_description': 'de hulpofficier van justitie bij de voorgeleiding beveelt', 'action': {'text': 'bij de voorgeleiding beveelt'}, 'actor': {'text': 'die', 'reference': 'de hulpofficier van justitie'}, 'object': None}, {'event_description': 'in afwijking van artikel 27e eerste lid geeft de hulpofficier van justitie spoedig kennis van de vrijheidsbeneming aan de ouders', 'action': {'text': 'in afwijking van artikel 27e eerste lid geeft aan de ouders'}, 'actor': {'text': 'de hulpofficier van justitie', 'reference': None}, 'object': {'text': 'spoedig kennis van de vrijheidsbeneming', 'reference': None}}, {'event_description': 'de ouders ontvangen daarbij zo spoedig mogelijk een mededeling van rechten', 'action': {'text': 'ontvangen daarbij zo spoedig mogelijk'}, 'actor': {'text': 'de ouders', 'reference': None}, 'object': {'text': 'een mededeling van rechten', 'reference': None}}, {'event_description': 'voogd ontvangen daarbij zo spoedig mogelijk een mededeling van rechten', 'action': {'text': 'ontvangen daarbij zo spoedig mogelijk'}, 'actor': {'text': 'voogd', 'reference': None}, 'object': {'text': 'een mededeling van rechten', 'reference': None}}, {'event_description': 'de mededeling van rechten blijft achterwege wanneer', 'action': {'text': 'blijft achterwege wanneer'}, 'actor': {'text': 'de mededeling van rechten', 'reference': None}, 'object': None}, {'event_description': 'de mededeling van rechten in strijd is met de belangen van de verdachte', 'action': {'text': 'in strijd is met de belangen van de verdachte'}, 'actor': {'text': 'deze', 'reference': 'de mededeling van rechten'}, 'object': None}, {'event_description': 'de ouders na redelijke inspanning niet kunnen worden bereikt', 'action': {'text': 'omdat na redelijke inspanning niet kunnen worden bereikt'}, 'actor': None, 'object': {'text': 'de ouders', 'reference': None}}, {'event_description': 'voogd na redelijke inspanning niet kunnen worden bereikt', 'action': {'text': 'omdat na redelijke inspanning niet kunnen worden bereikt'}, 'actor': None, 'object': {'text': 'voogd', 'reference': None}}, {'event_description': 'de ouders onbekend zijn', 'action': {'text': 'onbekend zijn'}, 'actor': None, 'object': {'text': 'de ouders', 'reference': None}}, {'event_description': 'voogd onbekend zijn', 'action': {'text': 'onbekend zijn'}, 'actor': None, 'object': {'text': 'voogd', 'reference': None}}, {'event_description': 'de mededeling niet mogelijk is', 'action': {'text': 'niet mogelijk is'}, 'actor': {'text': 'de mededeling', 'reference': None}, 'object': None}, {'event_description': 'in het geval wordt de mededeling gedaan aan een vertrouwenspersoon', 'action': {'text': 'in het geval wordt gedaan aan een vertrouwenspersoon'}, 'actor': None, 'object': {'text': 'de mededeling', 'reference': None}}, {'event_description': 'de verdachte geen vertrouwenspersoon heeft aangewezen', 'action': {'text': 'wanneer heeft aangewezen'}, 'actor': {'text': 'de verdachte', 'reference': None}, 'object': {'text': 'geen vertrouwenspersoon', 'reference': None}}, {'event_description': 'wordt de mededeling gedaan aan de raad voor de kinderbescherming', 'action': {'text': 'wordt gedaan aan de raad voor de kinderbescherming'}, 'actor': None, 'object': {'text': 'de mededeling', 'reference': None}}, {'event_description': 'ontvangen voogd de mededeling alsnog', 'action': {'text': 'ontvangen alsnog'}, 'actor': {'text': 'voogd', 'reference': None}, 'object': {'text': 'de mededeling', 'reference': None}}, {'event_description': 'de omstandigheden ophouden bestaan', 'action': {'text': 'indien ophouden te bestaan'}, 'actor': {'text': 'de omstandigheden', 'reference': None}, 'object': None}, {'event_description': 'ontvangen de ouders de mededeling alsnog', 'action': {'text': 'ontvangen alsnog'}, 'actor': {'text': 'de ouders', 'reference': None}, 'object': {'text': 'de mededeling', 'reference': None}}]\n"
     ]
    }
   ],
   "source": [
    "document_data = load_data(\"article_of_interest.txt\")\n",
    "\n",
    "\n",
    "document_data[\"sentence_information\"] = document_data[\"content\"].apply(\n",
    "    extract_events_for_article\n",
    ")\n",
    "\n",
    "document_data[\"sentence_information\"] = document_data[\"sentence_information\"].apply(\n",
    "    marker_detection\n",
    ")\n",
    "\n",
    "document_data[\"sentence_information\"] = document_data[\"sentence_information\"].apply(\n",
    "    detect_compound_indicators\n",
    ")\n",
    "\n",
    "document_data[\"sentence_information\"] = document_data[\"sentence_information\"].apply(\n",
    "    add_implicit_markers\n",
    ")\n",
    "\n",
    "document_data[\"sentence_information\"] = document_data[\"sentence_information\"].apply(\n",
    "    correct_order\n",
    ")\n",
    "\n",
    "document_data[\"sentence_information\"] = document_data[\"sentence_information\"].apply(\n",
    "    combine_actions\n",
    ")\n",
    "\n",
    "document_data[\"sentence_information\"] = document_data[\"sentence_information\"].apply(\n",
    "    determine_inter_action_links\n",
    ")\n",
    "\n",
    "\n",
    "# Print the ChatGPT input\n",
    "document_data[\"reduced_sentence_information\"] = document_data[\n",
    "    \"sentence_information\"\n",
    "].apply(generate_chat_gpt_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original event descriptions used to make the flow.\n",
    "original_event_list = []\n",
    "for sentence in document_data.at[0, \"sentence_information\"]:\n",
    "    for event in sentence[\"events\"]:\n",
    "        original_event_list.append(event)\n",
    "\n",
    "# Copy the corrected event descriptions form ChatGPT.\n",
    "chat_gpt_output_step_1 = [\n",
    "    {\n",
    "        \"event_description\": \"De verdachte wordt opgehouden voor onderzoek\",\n",
    "        \"action\": {\"text\": \"dat wordt opgehouden voor onderzoek\"},\n",
    "        \"actor\": None,\n",
    "        \"object\": {\"text\": \"de verdachte\", \"reference\": None},\n",
    "        \"original_event_id\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De hulpofficier van justitie bij de voorgeleiding beveelt\",\n",
    "        \"action\": {\"text\": \"bij de voorgeleiding beveelt\"},\n",
    "        \"actor\": {\"text\": \"die\", \"reference\": \"de hulpofficier van justitie\"},\n",
    "        \"object\": None,\n",
    "        \"original_event_id\": 2,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"In afwijking van artikel 27e, eerste lid, geeft de hulpofficier van justitie zo spoedig mogelijk kennis van de vrijheidsbeneming en van de redenen daarvan aan de ouders of voogd.\",\n",
    "        \"action\": {\n",
    "            \"text\": \"in afwijking van artikel 27e eerste lid geeft aan de ouders of voogd\"\n",
    "        },\n",
    "        \"actor\": {\"text\": \"de hulpofficier van justitie\", \"reference\": None},\n",
    "        \"object\": {\n",
    "            \"text\": \"spoedig kennis van de vrijheidsbeneming en van de redenen daarvan\",\n",
    "            \"reference\": None,\n",
    "        },\n",
    "        \"original_event_id\": 3,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De ouders ontvangen daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.\",\n",
    "        \"action\": {\"text\": \"ontvangen daarbij zo spoedig mogelijk\"},\n",
    "        \"actor\": {\"text\": \"de ouders\", \"reference\": None},\n",
    "        \"object\": {\n",
    "            \"text\": \"een mededeling van rechten zoals bedoeld in artikel 488aa\",\n",
    "            \"reference\": None,\n",
    "        },\n",
    "        \"original_event_id\": 4,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De voogd ontvangt daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.\",\n",
    "        \"action\": {\"text\": \"ontvangen daarbij zo spoedig mogelijk\"},\n",
    "        \"actor\": {\"text\": \"voogd\", \"reference\": None},\n",
    "        \"object\": {\n",
    "            \"text\": \"een mededeling van rechten zoals bedoeld in artikel 488aa\",\n",
    "            \"reference\": None,\n",
    "        },\n",
    "        \"original_event_id\": 5,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De mededeling van rechten blijft achterwege onder de omstandigheden gespecificeerd in lid 2.\",\n",
    "        \"action\": {\"text\": \"blijft achterwege wanneer\"},\n",
    "        \"actor\": {\"text\": \"de mededeling van rechten\", \"reference\": None},\n",
    "        \"object\": None,\n",
    "        \"original_event_id\": 6,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De mededeling van rechten is in strijd met de belangen van de verdachte.\",\n",
    "        \"action\": {\"text\": \"in strijd is met de belangen van de verdachte\"},\n",
    "        \"actor\": {\"text\": \"deze\", \"reference\": \"de mededeling van rechten\"},\n",
    "        \"object\": None,\n",
    "        \"original_event_id\": 7,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De ouders kunnen na redelijke inspanning niet worden bereikt voor de mededeling van rechten.\",\n",
    "        \"action\": {\"text\": \"omdat na redelijke inspanning niet kunnen worden bereikt\"},\n",
    "        \"actor\": None,\n",
    "        \"object\": {\"text\": \"de ouders\", \"reference\": None},\n",
    "        \"original_event_id\": 8,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De voogd kan na redelijke inspanning niet worden bereikt voor de mededeling van rechten.\",\n",
    "        \"action\": {\"text\": \"omdat na redelijke inspanning niet kunnen worden bereikt\"},\n",
    "        \"actor\": None,\n",
    "        \"object\": {\"text\": \"voogd\", \"reference\": None},\n",
    "        \"original_event_id\": 9,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De ouders zijn onbekend waardoor de mededeling van rechten niet gedaan kan worden.\",\n",
    "        \"action\": {\"text\": \"onbekend zijn\"},\n",
    "        \"actor\": None,\n",
    "        \"object\": {\"text\": \"de ouders\", \"reference\": None},\n",
    "        \"original_event_id\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De voogd is onbekend waardoor de mededeling van rechten niet gedaan kan worden.\",\n",
    "        \"action\": {\"text\": \"onbekend zijn\"},\n",
    "        \"actor\": None,\n",
    "        \"object\": {\"text\": \"voogd\", \"reference\": None},\n",
    "        \"original_event_id\": 11,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"Het doen van de mededeling is niet mogelijk onder de gespecificeerde omstandigheden.\",\n",
    "        \"action\": {\"text\": \"niet mogelijk is\"},\n",
    "        \"actor\": {\"text\": \"de mededeling\", \"reference\": None},\n",
    "        \"object\": None,\n",
    "        \"original_event_id\": 12,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"Indien de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan een vertrouwenspersoon.\",\n",
    "        \"action\": {\"text\": \"in het geval wordt gedaan aan een vertrouwenspersoon\"},\n",
    "        \"actor\": None,\n",
    "        \"object\": {\"text\": \"de mededeling\", \"reference\": None},\n",
    "        \"original_event_id\": 13,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"Als de verdachte geen vertrouwenspersoon heeft aangewezen, wordt de mededeling gedaan aan de raad voor de kinderbescherming.\",\n",
    "        \"action\": {\"text\": \"wanneer heeft aangewezen\"},\n",
    "        \"actor\": {\"text\": \"de verdachte\", \"reference\": None},\n",
    "        \"object\": {\"text\": \"geen vertrouwenspersoon\", \"reference\": None},\n",
    "        \"original_event_id\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"Als de verdachte geen vertrouwenspersoon heeft aangewezen en de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan de raad voor de kinderbescherming.\",\n",
    "        \"action\": {\"text\": \"wordt gedaan aan de raad voor de kinderbescherming\"},\n",
    "        \"actor\": None,\n",
    "        \"object\": {\"text\": \"de mededeling\", \"reference\": None},\n",
    "        \"original_event_id\": 15,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"De voogd ontvangt de mededeling alsnog indien de omstandigheden veranderen.\",\n",
    "        \"action\": {\"text\": \"ontvangen alsnog\"},\n",
    "        \"actor\": {\"text\": \"voogd\", \"reference\": None},\n",
    "        \"object\": {\"text\": \"de mededeling\", \"reference\": None},\n",
    "        \"original_event_id\": 16,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"Indien de eerder belemmerende omstandigheden ophouden te bestaan, ontvangen de ouders de mededeling alsnog.\",\n",
    "        \"action\": {\"text\": \"indien ophouden te bestaan\"},\n",
    "        \"actor\": {\"text\": \"de omstandigheden\", \"reference\": None},\n",
    "        \"object\": {\"text\": \"de mededeling\", \"reference\": None},\n",
    "        \"original_event_id\": 17,\n",
    "    },\n",
    "    {\n",
    "        \"event_description\": \"Indien de omstandigheden, bedoeld in het tweede lid, ophouden te bestaan, ontvangen de ouders de mededeling alsnog.\",\n",
    "        \"action\": {\"text\": \"ontvangen alsnog\"},\n",
    "        \"actor\": {\"text\": \"de ouders\", \"reference\": None},\n",
    "        \"object\": {\"text\": \"de mededeling\", \"reference\": None},\n",
    "        \"original_event_id\": 18,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from_event': 'start', 'to_event': 'De verdachte wordt opgehouden voor onderzoek'}, {'from_event': 'De verdachte wordt opgehouden voor onderzoek', 'to_event': 'De hulpofficier van justitie bij de voorgeleiding beveelt'}, {'from_event': 'De hulpofficier van justitie bij de voorgeleiding beveelt', 'to_event': 'In afwijking van artikel 27e, eerste lid, geeft de hulpofficier van justitie zo spoedig mogelijk kennis van de vrijheidsbeneming en van de redenen daarvan aan de ouders of voogd.'}, {'from_event': 'In afwijking van artikel 27e, eerste lid, geeft de hulpofficier van justitie zo spoedig mogelijk kennis van de vrijheidsbeneming en van de redenen daarvan aan de ouders of voogd.', 'to_event': 'of 0'}, {'from_event': 'of 0', 'to_event': 'De ouders ontvangen daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.'}, {'from_event': 'of 0', 'to_event': 'De voogd ontvangt daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.'}, {'from_event': 'De ouders ontvangen daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.', 'to_event': 'join 1'}, {'from_event': 'De voogd ontvangt daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.', 'to_event': 'join 1'}, {'from_event': 'join 1', 'to_event': 'De mededeling van rechten blijft achterwege onder de omstandigheden gespecificeerd in lid 2.'}, {'from_event': 'De mededeling van rechten blijft achterwege onder de omstandigheden gespecificeerd in lid 2.', 'to_event': 'De mededeling van rechten is in strijd met de belangen van de verdachte.'}, {'from_event': 'De mededeling van rechten is in strijd met de belangen van de verdachte.', 'to_event': 'of 2'}, {'from_event': 'of 2', 'to_event': 'De ouders kunnen na redelijke inspanning niet worden bereikt voor de mededeling van rechten.'}, {'from_event': 'of 2', 'to_event': 'De voogd kan na redelijke inspanning niet worden bereikt voor de mededeling van rechten.'}, {'from_event': 'of 2', 'to_event': 'De ouders zijn onbekend waardoor de mededeling van rechten niet gedaan kan worden.'}, {'from_event': 'of 2', 'to_event': 'De voogd is onbekend waardoor de mededeling van rechten niet gedaan kan worden.'}, {'from_event': 'De ouders kunnen na redelijke inspanning niet worden bereikt voor de mededeling van rechten.', 'to_event': 'join 3'}, {'from_event': 'De voogd kan na redelijke inspanning niet worden bereikt voor de mededeling van rechten.', 'to_event': 'join 3'}, {'from_event': 'De ouders zijn onbekend waardoor de mededeling van rechten niet gedaan kan worden.', 'to_event': 'join 3'}, {'from_event': 'De voogd is onbekend waardoor de mededeling van rechten niet gedaan kan worden.', 'to_event': 'join 3'}, {'from_event': 'join 3', 'to_event': 'Het doen van de mededeling is niet mogelijk onder de gespecificeerde omstandigheden.'}, {'from_event': 'Het doen van de mededeling is niet mogelijk onder de gespecificeerde omstandigheden.', 'to_event': 'Indien de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan een vertrouwenspersoon.'}, {'from_event': 'Indien de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan een vertrouwenspersoon.', 'to_event': 'Als de verdachte geen vertrouwenspersoon heeft aangewezen, wordt de mededeling gedaan aan de raad voor de kinderbescherming.'}, {'from_event': 'Als de verdachte geen vertrouwenspersoon heeft aangewezen, wordt de mededeling gedaan aan de raad voor de kinderbescherming.', 'to_event': 'Als de verdachte geen vertrouwenspersoon heeft aangewezen en de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan de raad voor de kinderbescherming.'}, {'from_event': 'Als de verdachte geen vertrouwenspersoon heeft aangewezen en de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan de raad voor de kinderbescherming.', 'to_event': 'of 4'}, {'from_event': 'of 4', 'to_event': 'De voogd ontvangt de mededeling alsnog indien de omstandigheden veranderen.'}, {'from_event': 'of 4', 'to_event': 'Indien de omstandigheden, bedoeld in het tweede lid, ophouden te bestaan, ontvangen de ouders de mededeling alsnog.'}, {'from_event': 'De voogd ontvangt de mededeling alsnog indien de omstandigheden veranderen.', 'to_event': 'join 5'}, {'from_event': 'Indien de omstandigheden, bedoeld in het tweede lid, ophouden te bestaan, ontvangen de ouders de mededeling alsnog.', 'to_event': 'join 5'}, {'from_event': 'join 5', 'to_event': 'Indien de eerder belemmerende omstandigheden ophouden te bestaan, ontvangen de ouders de mededeling alsnog.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'from_event': 'start',\n",
       "  'to_event': 'De verdachte wordt opgehouden voor onderzoek'},\n",
       " {'from_event': 'De verdachte wordt opgehouden voor onderzoek',\n",
       "  'to_event': 'De hulpofficier van justitie bij de voorgeleiding beveelt'},\n",
       " {'from_event': 'De hulpofficier van justitie bij de voorgeleiding beveelt',\n",
       "  'to_event': 'In afwijking van artikel 27e, eerste lid, geeft de hulpofficier van justitie zo spoedig mogelijk kennis van de vrijheidsbeneming en van de redenen daarvan aan de ouders of voogd.'},\n",
       " {'from_event': 'In afwijking van artikel 27e, eerste lid, geeft de hulpofficier van justitie zo spoedig mogelijk kennis van de vrijheidsbeneming en van de redenen daarvan aan de ouders of voogd.',\n",
       "  'to_event': 'of 0'},\n",
       " {'from_event': 'of 0',\n",
       "  'to_event': 'De ouders ontvangen daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.'},\n",
       " {'from_event': 'of 0',\n",
       "  'to_event': 'De voogd ontvangt daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.'},\n",
       " {'from_event': 'De ouders ontvangen daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.',\n",
       "  'to_event': 'join 1'},\n",
       " {'from_event': 'De voogd ontvangt daarbij zo spoedig mogelijk een mededeling van rechten zoals bedoeld in artikel 488aa.',\n",
       "  'to_event': 'join 1'},\n",
       " {'from_event': 'join 1',\n",
       "  'to_event': 'De mededeling van rechten blijft achterwege onder de omstandigheden gespecificeerd in lid 2.'},\n",
       " {'from_event': 'De mededeling van rechten blijft achterwege onder de omstandigheden gespecificeerd in lid 2.',\n",
       "  'to_event': 'De mededeling van rechten is in strijd met de belangen van de verdachte.'},\n",
       " {'from_event': 'De mededeling van rechten is in strijd met de belangen van de verdachte.',\n",
       "  'to_event': 'of 2'},\n",
       " {'from_event': 'of 2',\n",
       "  'to_event': 'De ouders kunnen na redelijke inspanning niet worden bereikt voor de mededeling van rechten.'},\n",
       " {'from_event': 'of 2',\n",
       "  'to_event': 'De voogd kan na redelijke inspanning niet worden bereikt voor de mededeling van rechten.'},\n",
       " {'from_event': 'of 2',\n",
       "  'to_event': 'De ouders zijn onbekend waardoor de mededeling van rechten niet gedaan kan worden.'},\n",
       " {'from_event': 'of 2',\n",
       "  'to_event': 'De voogd is onbekend waardoor de mededeling van rechten niet gedaan kan worden.'},\n",
       " {'from_event': 'De ouders kunnen na redelijke inspanning niet worden bereikt voor de mededeling van rechten.',\n",
       "  'to_event': 'join 3'},\n",
       " {'from_event': 'De voogd kan na redelijke inspanning niet worden bereikt voor de mededeling van rechten.',\n",
       "  'to_event': 'join 3'},\n",
       " {'from_event': 'De ouders zijn onbekend waardoor de mededeling van rechten niet gedaan kan worden.',\n",
       "  'to_event': 'join 3'},\n",
       " {'from_event': 'De voogd is onbekend waardoor de mededeling van rechten niet gedaan kan worden.',\n",
       "  'to_event': 'join 3'},\n",
       " {'from_event': 'join 3',\n",
       "  'to_event': 'Het doen van de mededeling is niet mogelijk onder de gespecificeerde omstandigheden.'},\n",
       " {'from_event': 'Het doen van de mededeling is niet mogelijk onder de gespecificeerde omstandigheden.',\n",
       "  'to_event': 'Indien de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan een vertrouwenspersoon.'},\n",
       " {'from_event': 'Indien de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan een vertrouwenspersoon.',\n",
       "  'to_event': 'Als de verdachte geen vertrouwenspersoon heeft aangewezen, wordt de mededeling gedaan aan de raad voor de kinderbescherming.'},\n",
       " {'from_event': 'Als de verdachte geen vertrouwenspersoon heeft aangewezen, wordt de mededeling gedaan aan de raad voor de kinderbescherming.',\n",
       "  'to_event': 'Als de verdachte geen vertrouwenspersoon heeft aangewezen en de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan de raad voor de kinderbescherming.'},\n",
       " {'from_event': 'Als de verdachte geen vertrouwenspersoon heeft aangewezen en de ouders of voogd niet bereikt kunnen worden, wordt de mededeling gedaan aan de raad voor de kinderbescherming.',\n",
       "  'to_event': 'of 4'},\n",
       " {'from_event': 'of 4',\n",
       "  'to_event': 'De voogd ontvangt de mededeling alsnog indien de omstandigheden veranderen.'},\n",
       " {'from_event': 'of 4',\n",
       "  'to_event': 'Indien de omstandigheden, bedoeld in het tweede lid, ophouden te bestaan, ontvangen de ouders de mededeling alsnog.'},\n",
       " {'from_event': 'De voogd ontvangt de mededeling alsnog indien de omstandigheden veranderen.',\n",
       "  'to_event': 'join 5'},\n",
       " {'from_event': 'Indien de omstandigheden, bedoeld in het tweede lid, ophouden te bestaan, ontvangen de ouders de mededeling alsnog.',\n",
       "  'to_event': 'join 5'},\n",
       " {'from_event': 'join 5',\n",
       "  'to_event': 'Indien de eerder belemmerende omstandigheden ophouden te bestaan, ontvangen de ouders de mededeling alsnog.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_data[\"flows\"] = document_data[\"sentence_information\"].apply(build_flows)\n",
    "\n",
    "generate_chat_gpt_output(document_data.at[0,\"flows\"], chat_gpt_output_step_1, original_event_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
